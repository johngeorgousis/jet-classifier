
OLD ML Dataset create_dataset

# # # Load Previously saved data
# # sdata = pd.read_pickle('sdata')
# # bdata = pd.read_pickle('bdata')

# # Import, Preprocess, Create Dataset
# sdata = create_dataset('data_signal.dat')
# bdata = create_dataset('data_background.dat')

# # Save Datasets
# pd.DataFrame(sdata).to_pickle('sdata')
# pd.DataFrame(sdata).to_pickle('bdata')

# stest = pd.read_pickle('sdata')
# btest = pd.read_pickle('bdata')

# # Concat and Suffle
# data = np.concatenate((sdata, bdata), axis=0)
# data = sklearn.utils.shuffle(data)

# num_of_examples = data.shape[0]
# print('Total Events:', num_of_examples)

# # Separate Examples and Labels
# data= pd.DataFrame(data, columns=['examples', 'labels'])
# examples= data['examples']         
# labels = data['labels']    

# from sklearn.model_selection import train_test_split

# train_examples, test_examples, train_labels, test_labels = train_test_split(examples, labels, test_size=0.15, random_state=42)

# train_examples, val_examples, train_labels, val_labels = train_test_split(train_examples, train_labels, test_size=0.18, random_state=42)

# print('Train: ', train_examples.shape, train_labels.shape)
# print('Val: ', val_examples.shape, val_labels.shape)
# print('Test: ', test_examples.shape, test_labels.shape)

# train_data = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))
# val_data = tf.data.Dataset.from_tensor_slices((val_examples, val_labels))
# test_data = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))

# print(train_data)
# print(val_data)
# print(test_data)





















Lame NN

# model = tf.keras.Sequential([
#            tf.keras.layers.Flatten(input_shape=(batch_size, pixels, pixels)),
#            tf.keras.layers.Dense(256, activation = 'relu'),
#            tf.keras.layers.Dropout(0.2),
#            tf.keras.layers.Dense(128, activation = 'relu'),
#            tf.keras.layers.Dropout(0.2),
#            tf.keras.layers.Dense(64, activation = 'relu'),
#            tf.keras.layers.Dropout(0.2),
#            tf.keras.layers.Dense(2, activation = 'softmax')
# ])













# ⭐(Ignore) Step 0: Read the data (tar.gz file) & Explore it
**Read**

As a first step, we unzipped the tar.gz file into a .dat file using 7-zip. 
Then, we convert the .dat file into a string and then into a DataFrame.

.strip() --> remove spaces on the sides

.split() --> separate values by spaces (otherwise we'd get a single conlumn)

**Explore**

**Physics**

Jonas: "The file was produced from a simulation of pp->tt~H where the top decays hadronically
and the anti-top decays leptonically. <br /> I selected events with exactly 1 fat jet with R=1.5."


**Notes**
- The rows represent events (of 1 fat jet each, R = 1.5) 
- The first column represents the number of constituents of the jet  
- The following columns represent the coordinates of the constituents, η, φ, pT, cycling in that order. <br />(e.g. columns 1, 2, 3 are η, φ, pT for the 1st constituent, columns 4, 5, 6 are η, φ, pT for the 2nd constituent etc.)


- -infinity < η < infinity 
- -π < φ < π
- pT[GeV] > 0



# Convert .dat file into string (list comprehension)
datContent = [i.strip().split() for i in open("tth_semihad.dat").readlines()]

# Convert list into DataFrame
mydata = pd.DataFrame(datContent)





# # Display the data
# mydata = mydata.rename(columns={0: 'Const'})
# display(mydata.head())

# # Print statements
# events = mydata.shape[0]
# print('There are {} rows (events).'.format(events))
# print('The maximum number of constituents in an event is {}.'.format((mydata.shape[1] - 1) // 3))

## Display data types
#print('\nData Types: \n', mydata.dtypes)

## Descriptive statistics on data
#mydata.describe()











