{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FUNCTIONS import *\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn.discriminant_analysis\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-1159c73f2f91>\", line 2, in <module>\n",
      "    data_s = np.array(pd.read_csv('data/data_s_180k').iloc[:, 1:])\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 676, in parser_f\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 448, in _read\n",
      "    parser = TextFileReader(fp_or_buf, **kwds)\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 880, in __init__\n",
      "    self._make_engine(self.engine)\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1114, in _make_engine\n",
      "    self._engine = CParserWrapper(self.f, **self.options)\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\", line 1891, in __init__\n",
      "    self._reader = parsers.TextReader(src, **kwds)\n",
      "  File \"pandas\\_libs\\parsers.pyx\", line 374, in pandas._libs.parsers.TextReader.__cinit__\n",
      "  File \"pandas\\_libs\\parsers.pyx\", line 674, in pandas._libs.parsers.TextReader._setup_parser_source\n",
      "FileNotFoundError: [Errno 2] File data/data_s_180k does not exist: 'data/data_s_180k'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'FileNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\johng\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File data/data_s_180k does not exist: 'data/data_s_180k'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "data_s = np.array(pd.read_csv('data/data_s_180k').iloc[:, 1:])\n",
    "data_b = np.array(pd.read_csv('data/data_b_180k').iloc[:, 1:])\n",
    "\n",
    "# Slice & Save\n",
    "events_no = int(1e5)\n",
    "events_no = 2000\n",
    "data_s = data_s[0:events_no*40, 0:40]\n",
    "data_b = data_b[0:events_no*40, 0:40]\n",
    "\n",
    "# # Save Datasets\n",
    "# pd.DataFrame(data_s).to_csv('data_s_1000')\n",
    "# pd.DataFrame(data_b).to_csv('data_b_1000')\n",
    "\n",
    "train_examples, train_labels, val_examples, val_labels, test_examples, test_labels = preprocess_ML_sklearn(data_s, data_b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "svm = SVC(random_state=42)\n",
    "KNN = sklearn.neighbors.KNeighborsClassifier()\n",
    "naive_bayes = MultinomialNB()\n",
    "BDT = sklearn.ensemble.GradientBoostingClassifier()\n",
    "bagging = BaggingClassifier(random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import time\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "alpha = [0, 0.001, 0.01, 0.1, 0.5, 1, 1.5, 2, 5, 10, 15, 50]\n",
    "\n",
    "# Bagging\n",
    "n_estimators_bagging = [10, 20, 50, 100, 200, 300]\n",
    "max_samples_bagging = [0.1, 0.2, 0.5, 1.0, 2.0, 3.0]\n",
    "max_features_bagging = [0.5, 1.0, 2.0, 3.0, 50.0, 200]\n",
    "\n",
    "# Random Forests\n",
    "bootstrap = [True, False]\n",
    "criterion = ['gini', 'entropy']\n",
    "n_estimators_rf = [10, 50, 100, 200, 500, 1000, 2000, 5000]\n",
    "max_depth_rf = [10, 20, 40, 50, 70, 100, None]\n",
    "max_features_rf = ['auto', 'sqrt']\n",
    "min_samples_leaf_rf = [1, 2, 4, 8, 15, 40]\n",
    "min_samples_split_rf = [2, 5, 10, 30, 50]\n",
    "\n",
    "# SVC\n",
    "kernel = ['rbf']\n",
    "c_parameter = [0.1, 1, 10]\n",
    "gamma = [0.001, 0.01, 0.1, 1, 5]\n",
    "\n",
    "\n",
    "# KNN\n",
    "n_neighbors=[1, 2, 5, 10, 20, 30]\n",
    "leaf_size_KNN=[1, 5, 10, 20, 30, 50]\n",
    "p=[1, 2]\n",
    "\n",
    "# GradientBoosting\n",
    "n_estimators_GB = [10, 50, 100, 500, 2000]\n",
    "learning_rate_GB = [0.01, 0.1, 1, 3]\n",
    "min_samples_split_GB = [1, 2, 10]\n",
    "min_samples_leaf_GB = [1, 5, 10]\n",
    "max_depth_GB = [1, 3, 10, 50]\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "naive_bayes_parameters = {'alpha': alpha}\n",
    "bagging_parameters = {'n_estimators': n_estimators_bagging, 'max_features': max_features_bagging, 'max_samples': max_samples_bagging}\n",
    "random_forest_parameters = {'bootstrap': bootstrap, 'criterion': criterion, 'max_features':max_features_rf, 'n_estimators': n_estimators_rf, 'max_depth': max_depth_rf, 'min_samples_leaf': min_samples_leaf_rf, 'min_samples_split': min_samples_split_rf}\n",
    "svm_parameters = {'kernel': kernel, 'C': c_parameter, 'gamma': gamma}\n",
    "GB_parameters = {'n_estimators_': n_estimators_GB, 'learning_rate': learning_rate_GB, 'min_samples_split': min_samples_split_GB, 'min_samples_split': min_samples_split_GB, 'min_samples_leaf': min_samples_leaf_GB, 'max_depth': max_depth_GB}\n",
    "KNN_parameters = {'n_neighbors': n_neighbors, 'leaf_size': leaf_size_KNN, 'p': p}\n",
    "\n",
    "# Scoring object using accuracy\n",
    "scorer = make_scorer(accuracy)\n",
    "\n",
    "\n",
    "clfs_param =[(naive_bayes, naive_bayes_parameters), \n",
    "             (random_forest, random_forest_parameters),\n",
    "             (KNN, KNN_parameters),\n",
    "             (BDT, GB_parameters),\n",
    "             (bagging, bagging_parameters),\n",
    "             (svm, svm_parameters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store models\n",
    "models = []\n",
    "unopt_accuracy = []\n",
    "accuracies = []\n",
    "\n",
    "# Perform grid search\n",
    "for clf, parameter in clfs_param:\n",
    "    print('\\n{}\\n'.format(clf.__class__.__name__))\n",
    "    \n",
    "    grid_obj = GridSearchCV(clf, parameter, scoring=scorer, n_jobs = 6)\n",
    "    \n",
    "    # Perform grid search\n",
    "    start = time.time()\n",
    "    grid_fit = grid_obj.fit(train_examples, train_labels)\n",
    "    end = time.time()\n",
    "    print('Time to tune: {}s'.format(round(end - start), 2))\n",
    "    \n",
    "    # Get best estimator\n",
    "    best_clf = grid_fit.best_estimator_\n",
    "    models.append(best_clf)\n",
    "    \n",
    "    # Make predictions using the unoptimized and model\n",
    "    start = time.time()\n",
    "    predictions = (clf.fit(train_examples, train_labels)).predict(val_examples)\n",
    "    best_predictions = best_clf.predict(val_examples)\n",
    "    \n",
    "    predictions_train = (clf.fit(train_examples, train_labels)).predict(train_examples)\n",
    "    best_predictions_train = best_clf.predict(train_examples)\n",
    "    end = time.time()\n",
    "    print('Time to fit-predict: {}s\\n'.format(round(end - start), 2))\n",
    "    \n",
    "    # Check hyperparameters\n",
    "    print('Unoptimised: {}\\n'.format(clf.get_params(deep = True)))\n",
    "    print('Optimised: {}\\n'.format(best_clf.get_params(deep = True)))\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"\\nUnoptimised-accuracy-training: {:.4f}\".format(accuracy_score(train_labels, predictions_train)))\n",
    "    print(\"Optimised-accuracy-training: {:.4f}\".format(accuracy_score(train_labels, best_predictions_train)))\n",
    "    \n",
    "    print(\"\\nUnoptimised-accuracy-validation: {:.4f}\".format(accuracy_score(val_labels, predictions)))\n",
    "    print(\"Optimised-accuracy-validation: {:.4f}\".format(accuracy_score(val_labels, best_predictions)))\n",
    "    \n",
    "    print('\\n\\n=============================================================================================')\n",
    "    \n",
    "    unopt_accuracy.append(accuracy_score(val_labels, predictions))\n",
    "    accuracies.append(accuracy_score(val_labels, best_predictions))\n",
    "    \n",
    "print('All unoptimised f1 (validation): {}'.format(unopt_f1))\n",
    "print('Best unoptimised f1 (validation): {}\\n'.format(max(unopt_f1)))\n",
    "print('All optimised f1 (validation): {}'.format(f1))\n",
    "print('Best optimised f1 (validation): {}'.format(max(f1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
