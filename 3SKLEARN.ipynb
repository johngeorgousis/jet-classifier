{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠êImports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "from processing_functions import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠ê Building Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîµ Step 1: Preprocess Data (Create Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "data_s = np.array(pd.read_csv('data_s_csv').iloc[:, 1:])\n",
    "data_b = np.array(pd.read_csv('data_b_csv').iloc[:, 1:])\n",
    "\n",
    "# # LOG\n",
    "# data_s = np.array(pd.read_csv('data_s_log_csv').iloc[:, 1:])\n",
    "# data_b = np.array(pd.read_csv('data_b_log_csv').iloc[:, 1:])\n",
    "\n",
    "# # NORMAL\n",
    "# data_s = np.array(pd.read_csv('data_s_normal_csv').iloc[:, 1:])\n",
    "# data_b = np.array(pd.read_csv('data_b_normal_csv').iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Events: 29449\n",
      "Total Labels: 29449\n",
      "Shape:  (29449, 1600)\n"
     ]
    }
   ],
   "source": [
    "# Create s&b labels\n",
    "slabels = np.ones(data_s.shape[0]//40)\n",
    "blabels = np.zeros(data_b.shape[0]//40)\n",
    "\n",
    "# Concatenate examples and labels\n",
    "data = np.concatenate((data_s, data_b), axis=0)\n",
    "labels = np.concatenate((slabels, blabels), axis=0)\n",
    "\n",
    "# Define useful quantities\n",
    "num_of_examples = data.shape[0] // 40     # divide by 40 because 1st dim is 40 * num_of_examples\n",
    "num_of_labels = labels.shape[0]\n",
    "print('Total Events:', num_of_examples)\n",
    "print('Total Labels:', num_of_labels)\n",
    "\n",
    "# Reshape examples (for sklearn)\n",
    "examples = data.reshape(num_of_examples, 1600)\n",
    "print('Shape: ', examples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  (20525, 1600) (20525,)\n",
      "Val:  (4506, 1600) (4506,)\n",
      "Test:  (4418, 1600) (4418,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_examples, test_examples, train_labels, test_labels = train_test_split(examples, labels, test_size=0.15, random_state=42)\n",
    "train_examples, val_examples, train_labels, val_labels = train_test_split(train_examples, train_labels, test_size=0.18, random_state=42)\n",
    "\n",
    "print('Train: ', train_examples.shape, train_labels.shape)\n",
    "print('Val: ', val_examples.shape, val_labels.shape)\n",
    "print('Test: ', test_examples.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîµ Step 2: Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Methods\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Trees\n",
    "from sklearn.naive_bayes import MultinomialNB    # Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB       # Gaussian Naive Bayes\n",
    "from sklearn.svm import SVC                      # SVM\n",
    "import random\n",
    "\n",
    "# Instantiate classes\n",
    "random.seed(42)\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes_g = GaussianNB()\n",
    "bagging = BaggingClassifier(random_state=42, n_jobs=6)\n",
    "random_forest = RandomForestClassifier(random_state=42, n_jobs=6)\n",
    "svm = SVC(random_state=42)\n",
    "adaboost = AdaBoostClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîµ Step 3: Hyperparameter Tuning (Hardcore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "alpha = [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2]\n",
    "\n",
    "# Bagging\n",
    "n_estimators_bagging = [10, 20, 30, 50, 100, 200, 300]\n",
    "max_samples_bagging = [0.1, 0.2, 0.5, 1.0, 2.0, 3.0, 5.0]\n",
    "max_features_bagging = [0.5, 1.0, 2.0, 3.0, 50.0, 200]\n",
    "\n",
    "# Random Forests\n",
    "criterion = ['gini', 'entropy']\n",
    "n_estimators_rf = [100, 150, 200]\n",
    "max_depth_rf = [None, 1, 3, 5, 10]\n",
    "min_samples_leaf_rf = [5, 10]\n",
    "min_samples_split_rf = [5, 10]\n",
    "\n",
    "\n",
    "# SVC\n",
    "kernel = ['rbf']\n",
    "c_parameter = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "gamma = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "\n",
    "# AdaBoost\n",
    "n_estimators_ada = [10, 30, 50, 100, 200, 500]\n",
    "learning_rate = [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "naive_bayes_parameters = {'alpha': alpha}\n",
    "naive_bayes_g_parameters = {'var_smoothing': np.logspace(0,-9, num=1000)}\n",
    "bagging_parameters = {'n_estimators': n_estimators_bagging, 'max_features': max_features_bagging, 'max_samples': max_samples_bagging}\n",
    "random_forest_parameters = {'n_estimators': n_estimators_rf, 'max_depth': max_depth_rf, 'min_samples_leaf': min_samples_leaf_rf, 'min_samples_split': min_samples_split_rf}\n",
    "svm_parameters = {'kernel': kernel, 'C': c_parameter, 'gamma': gamma}\n",
    "adaboost_parameters = {'n_estimators': n_estimators_ada, 'learning_rate': learning_rate}\n",
    "\n",
    "# Scoring object using accuracy\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "\n",
    "clfs_param =[(naive_bayes, naive_bayes_parameters), \n",
    "             (naive_bayes_g, naive_bayes_g_parameters), \n",
    "             (bagging, bagging_parameters), \n",
    "             (random_forest, random_forest_parameters), \n",
    "             (svm, svm_parameters), \n",
    "             (adaboost, adaboost_parameters)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîµ Step 3: Hyperparameter Tuning (Softcore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "alpha = [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2]\n",
    "\n",
    "# Bagging\n",
    "n_estimators_bagging = [10, 50, 100, 300]\n",
    "max_samples_bagging = [0.1, 1.0, 2.0, 5.0]\n",
    "max_features_bagging = [0.5, 3.0, 50.0, 200]\n",
    "\n",
    "# Random Forests\n",
    "criterion = ['gini', 'entropy']\n",
    "n_estimators_rf = [100, 150, 200]\n",
    "max_depth_rf = [None, 1, 3, 5, 10]\n",
    "min_samples_leaf_rf = [5, 10]\n",
    "min_samples_split_rf = [5, 10]\n",
    "\n",
    "\n",
    "# SVC\n",
    "kernel = ['rbf']\n",
    "c_parameter = [0.0001, 0.01, 0.1, 1, 10]\n",
    "gamma = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "\n",
    "# AdaBoost\n",
    "n_estimators_ada = [10, 30, 100, 500]\n",
    "learning_rate = [0.01, 0.1, 0.5, 1, 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "naive_bayes_parameters = {'alpha': alpha}\n",
    "naive_bayes_g_parameters = {'var_smoothing': np.logspace(0,-9, num=300)}\n",
    "bagging_parameters = {'n_estimators': n_estimators_bagging, 'max_features': max_features_bagging, 'max_samples': max_samples_bagging}\n",
    "random_forest_parameters = {'n_estimators': n_estimators_rf, 'max_depth': max_depth_rf, 'min_samples_leaf': min_samples_leaf_rf, 'min_samples_split': min_samples_split_rf}\n",
    "svm_parameters = {'kernel': kernel, 'C': c_parameter, 'gamma': gamma}\n",
    "adaboost_parameters = {'n_estimators': n_estimators_ada, 'learning_rate': learning_rate}\n",
    "\n",
    "# Scoring object using accuracy\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "\n",
    "clfs_param =[(naive_bayes, naive_bayes_parameters), \n",
    "             (naive_bayes_g, naive_bayes_g_parameters), \n",
    "             (bagging, bagging_parameters), \n",
    "             (random_forest, random_forest_parameters), \n",
    "             (adaboost, adaboost_parameters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MultinomialNB\n",
      "\n",
      "Time to tune: 4s\n",
      "Time to fit-predict: 0s\n",
      "\n",
      "Unoptimised: {'alpha': 1.0, 'class_prior': None, 'fit_prior': True}\n",
      "\n",
      "Optimised: {'alpha': 2, 'class_prior': None, 'fit_prior': True}\n",
      "\n",
      "\n",
      "Unoptimised-Accuracy-training: 0.7205\n",
      "Optimised-Accuracy-training: 0.7204\n",
      "\n",
      "Unoptimised-Accuracy-validation: 0.7022\n",
      "Optimised-Accuracy-validation: 0.7024\n",
      "\n",
      " \n",
      " \n",
      "=============================================================================================\n",
      "\n",
      "GaussianNB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create list to store models\n",
    "models = []\n",
    "unopt_accuracies = []\n",
    "accuracies = []\n",
    "\n",
    "# Perform grid search\n",
    "for clf, parameter in clfs_param:\n",
    "    print('\\n{}\\n'.format(clf.__class__.__name__))\n",
    "    \n",
    "    grid_obj = GridSearchCV(clf, parameter, scoring=scorer, n_jobs = 6)\n",
    "    \n",
    "    # Perform grid search\n",
    "    start = time()\n",
    "    grid_fit = grid_obj.fit(train_examples, train_labels)\n",
    "    end = time()\n",
    "    print('Time to tune: {}s'.format(round(end - start), 2))\n",
    "    \n",
    "    # Get best estimator\n",
    "    best_clf = grid_fit.best_estimator_\n",
    "    models.append(best_clf)\n",
    "    \n",
    "    # Make predictions using the unoptimized and model\n",
    "    start = time()\n",
    "    predictions = (clf.fit(train_examples, train_labels)).predict(val_examples)\n",
    "    best_predictions = best_clf.predict(val_examples)\n",
    "    \n",
    "    predictions_train = (clf.fit(train_examples, train_labels)).predict(train_examples)\n",
    "    best_predictions_train = best_clf.predict(train_examples)\n",
    "    end = time()\n",
    "    print('Time to fit-predict: {}s\\n'.format(round(end - start), 2))\n",
    "    \n",
    "    # Check hyperparameters\n",
    "    print('Unoptimised: {}\\n'.format(clf.get_params(deep = True)))\n",
    "    print('Optimised: {}\\n'.format(best_clf.get_params(deep = True)))\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"\\nUnoptimised-Accuracy-training: {:.4f}\".format(accuracy_score(train_labels, predictions_train)))\n",
    "    print(\"Optimised-Accuracy-training: {:.4f}\".format(accuracy_score(train_labels, best_predictions_train)))\n",
    "    \n",
    "    print(\"\\nUnoptimised-Accuracy-validation: {:.4f}\".format(accuracy_score(val_labels, predictions)))\n",
    "    print(\"Optimised-Accuracy-validation: {:.4f}\".format(accuracy_score(val_labels, best_predictions)))\n",
    "    \n",
    "    print('\\n \\n \\n=============================================================================================')\n",
    "    \n",
    "    unopt_accuracies.append(accuracy_score(val_labels, predictions))\n",
    "    accuracies.append(accuracy_score(val_labels, best_predictions))\n",
    "    \n",
    "print('All unoptimised accuracies (validation): {}'.format(unopt_accuracies))\n",
    "print('Best unoptimised accuracy (validation): {}\\n'.format(max(unopt_accuracies)))\n",
    "print('All optimised accuracies (validation): {}'.format(accuracies))\n",
    "print('Best optimised accuracy (validation): {}'.format(max(accuracies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîµ Step 4: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    \n",
    "    labels = model.predict(test_examples)\n",
    "    preds = test_labels\n",
    "    \n",
    "    cmx_non_normal = tf.math.confusion_matrix(labels, preds).numpy() # Create Confusion Matrix\n",
    "    cmx0 = cmx_non_normal[0] / cmx_non_normal[0].sum()\n",
    "    cmx1 = cmx_non_normal[1] / cmx_non_normal[1].sum()\n",
    "    cmx = np.stack((cmx0, cmx1), axis=0)\n",
    "\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(cmx, cmap=['skyblue', 'deepskyblue', 'dodgerblue', 'blue',  'darkblue'])\n",
    "\n",
    "    # xylabels and title\n",
    "    plt.title(remove_text_inside_brackets(str(model)))\n",
    "    plt.xlabel('PREDICTIONS')\n",
    "    plt.ylabel('LABELS')\n",
    "\n",
    "    # Label ticks\n",
    "    ax.set_xticklabels(['Background', 'Signal'])\n",
    "    ax.set_yticklabels(['Background', 'Signal'])\n",
    "    # Align ticks\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    plt.setp(ax.get_yticklabels(), rotation=90, ha=\"center\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Text Annotations for Blocks in CMX\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "\n",
    "            value = int(np.round(100*cmx[i, j], 0))\n",
    "\n",
    "            text = ax.text(j+0.5, \n",
    "                           i+0.5, \n",
    "                           value,\n",
    "                           ha=\"center\", \n",
    "                           va=\"center\", \n",
    "                           color=\"orangered\", \n",
    "                           fontsize = 20)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # # Print P(signal|signal) and P(signal|background)\n",
    "    # pss = cmx[1,1] / (cmx[1,1]+cmx[1,0])\n",
    "    # pbs = 1 - pss\n",
    "    # psb = cmx[0,1] / (cmx[0,1]+cmx[0,0])\n",
    "    # pbb = 1 - psb\n",
    "    # precision = cmx[1,1] / (cmx[1,1]+cmx[0,1])\n",
    "    # recall = cmx[1,1] / (cmx[1,1]+cmx[1,0])\n",
    "    # print('\\n')\n",
    "    # print('P(signal|signal) = {:.0f}%'.format(100*pss))\n",
    "    # print('P(signal|background) = {:.0f}%'.format(100*psb)) \n",
    "    # print('P(background|background) = {:.0f}%'.format(100*pbb))\n",
    "    # print('P(background|signal) = {:.0f}%'.format(100*pbs))\n",
    "    # print('Precision = {:.0f}'.format(precision*100))\n",
    "    # print('Recall = {:.0f}'.format(recall*100))\n",
    "    # print('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
