{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store models\n",
    "models = []\n",
    "unopt_accuracy = []\n",
    "accuracies = []\n",
    "\n",
    "# Perform grid search\n",
    "for clf, parameter in clfs_param:\n",
    "    print('\\n{}\\n'.format(clf.__class__.__name__))\n",
    "    \n",
    "    grid_obj = GridSearchCV(clf, parameter, scoring=scorer, n_jobs = 6)\n",
    "    \n",
    "    # Perform grid search\n",
    "    start = time.time()\n",
    "    grid_fit = grid_obj.fit(train_examples, train_labels)\n",
    "    end = time.time()\n",
    "    print('Time to tune: {}s'.format(round(end - start), 2))\n",
    "    \n",
    "    # Get best estimator\n",
    "    best_clf = grid_fit.best_estimator_\n",
    "    models.append(best_clf)\n",
    "    \n",
    "    # Make predictions using the unoptimized and model\n",
    "    start = time.time()\n",
    "    predictions = (clf.fit(train_examples, train_labels)).predict(val_examples)\n",
    "    best_predictions = best_clf.predict(val_examples)\n",
    "    \n",
    "    predictions_train = (clf.fit(train_examples, train_labels)).predict(train_examples)\n",
    "    best_predictions_train = best_clf.predict(train_examples)\n",
    "    end = time.time()\n",
    "    print('Time to fit-predict: {}s\\n'.format(round(end - start), 2))\n",
    "    \n",
    "    # Check hyperparameters\n",
    "    print('Unoptimised: {}\\n'.format(clf.get_params(deep = True)))\n",
    "    print('Optimised: {}\\n'.format(best_clf.get_params(deep = True)))\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"\\nUnoptimised-accuracy-training: {:.4f}\".format(accuracy_score(train_labels, predictions_train)))\n",
    "    print(\"Optimised-accuracy-training: {:.4f}\".format(accuracy_score(train_labels, best_predictions_train)))\n",
    "    \n",
    "    print(\"\\nUnoptimised-accuracy-validation: {:.4f}\".format(accuracy_score(val_labels, predictions)))\n",
    "    print(\"Optimised-accuracy-validation: {:.4f}\".format(accuracy_score(val_labels, best_predictions)))\n",
    "    \n",
    "    print('\\n\\n=============================================================================================')\n",
    "    \n",
    "    unopt_accuracy.append(accuracy_score(val_labels, predictions))\n",
    "    accuracies.append(accuracy_score(val_labels, best_predictions))\n",
    "    \n",
    "print('All unoptimised accuracy (validation): {}'.format(unopt_accuracy))\n",
    "print('Best unoptimised accuracy (validation): {}\\n'.format(max(unopt_accuracy)))\n",
    "print('All optimised accuracy (validation): {}'.format(accuracies))\n",
    "print('Best optimised accuracy (validation): {}'.format(max(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "alpha = [0, 0.001, 0.01, 0.1, 0.5, 1, 1.5, 2, 5, 10, 15, 20, 30, 40, 50, 80, 100, 150]\n",
    "\n",
    "# Random Forest\n",
    "criterion = ['gini', 'entropy']\n",
    "n_estimators_rf = [10, 50, 100, 200]\n",
    "max_depth_rf = [5, 20, 50, None]\n",
    "min_samples_leaf_rf = [1, 2, 15, 40]\n",
    "min_samples_split_rf = [2, 5, 10, 50]\n",
    "\n",
    "# KNN\n",
    "n_neighbors=[1, 3, 5, 12] \n",
    "weights_knn=['uniform', 'distance']\n",
    "leaf_size_knn=[10, 30, 60]\n",
    "p= [1, 2]\n",
    "\n",
    "\n",
    "# Bagging\n",
    "n_estimators_bagging = [10, 20, 50, 100, 200]\n",
    "max_samples_bagging = [0.1, 0.2, 1.0, 2.0, 3.0]\n",
    "max_features_bagging = [0.5, 1.0, 2.0, 50.0]\n",
    "\n",
    "\n",
    "# SVM\n",
    "c_parameter = [0.1, 1, 10]\n",
    "gamma = [0.001, 0.01, 0.1, 1, 5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "naive_bayes_parameters = {'alpha': alpha}\n",
    "random_forest_parameters = {'criterion': criterion, 'n_estimators': n_estimators_rf, 'max_depth': max_depth_rf, 'min_samples_leaf': min_samples_leaf_rf, 'min_samples_split': min_samples_split_rf}\n",
    "KNN_parameters = {'n_neighbors': n_neighbors, 'weights': weights_knn, 'leaf_size': leaf_size_knn, 'p': p}\n",
    "bagging_parameters = {'n_estimators': n_estimators_bagging, 'max_samples': max_samples_bagging, 'max_features': max_features_bagging}\n",
    "SVM_parameters = {'c_parameter': c_parameter, 'gamma': gamma}\n",
    "\n",
    "# Scoring object using accuracy\n",
    "scorer = make_scorer(accuracy_score)\n",
    "\n",
    "\n",
    "clfs_param =[(SVM, SVM_parameters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "random_forest = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "SVM = SVC(random_state=42)\n",
    "KNN = sklearn.neighbors.KNeighborsClassifier(n_jobs=-1)\n",
    "bagging = BaggingClassifier(random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "data_s = np.array(pd.read_csv('data/data_s_180k').iloc[:, 1:])\n",
    "data_b = np.array(pd.read_csv('data/data_b_180k').iloc[:, 1:])\n",
    "\n",
    "# Slice & Save\n",
    "events_no = int(50000)\n",
    "data_s = data_s[0:events_no*40, 0:40]\n",
    "data_b = data_b[0:events_no*40, 0:40]\n",
    "\n",
    "# # Save Datasets\n",
    "# pd.DataFrame(data_s).to_csv('data_s_1000')\n",
    "# pd.DataFrame(data_b).to_csv('data_b_1000')\n",
    "\n",
    "train_examples, train_labels, val_examples, val_labels, test_examples, test_labels = preprocess_ML_sklearn(data_s, data_b)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import display\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Trees\n",
    "from sklearn.naive_bayes import MultinomialNB    # Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB       # Gaussian Naive Bayes\n",
    "from sklearn.svm import SVC                      # SVM\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "print('TensorFlow Version: ', tf.__version__)\n",
    "                \n",
    "\n",
    "\n",
    "'''\n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW TENSORFLOW \n",
    "'''\n",
    "def create_dataset(file, pixels=40, R=1.5):\n",
    "    '''\n",
    "    Takes dat file of events\n",
    "    Labels events (background = 0, signal = 1)\n",
    "    Preprocessed events and turns into images\n",
    "    Returns 2d array where rows: events and columns: (image, label) \n",
    "    '''\n",
    "\n",
    "    image = np.zeros((pixels, pixels))                           # Define initial image\n",
    "    data = {}\n",
    "    a = 0\n",
    "    \n",
    "    with open(file) as infile:\n",
    "        for line in infile:\n",
    "\n",
    "            # Preprocessing\n",
    "            event = line.strip().split()\n",
    "            event = pd.Series(event)                         # Turn into Series\n",
    "            event = preprocess(event)                        # Preprocess\n",
    "            max1 = find_max1(event)                          # Extract maxima\n",
    "            event = center(event, max1)                      # Center \n",
    "            max2 = find_max2(event)                          # Extract maxima\n",
    "            event = rotate(event, max2)                      # Rotate \n",
    "            max3 = find_max3(event)                          # Extract maxima\n",
    "            event = flip(event, max3)                        # Flip \n",
    "            event = create_image(event, pixels=pixels, R=R)  # Create image\n",
    "            image = event                                    # Rename\n",
    "            #image /= np.sum(image)\n",
    "            #image /= np.amax(image)                          # Normalise final image between 0 and 1\n",
    "            #image = np.log(image)                            # Log image\n",
    "            \n",
    "            event=max1=max2=max3=None\n",
    "            \n",
    "            a += 1\n",
    "            data[a] = image\n",
    "            \n",
    "    data = list(data.values())\n",
    "    data = np.array(data)\n",
    "    data = np.reshape(data, (a*pixels, pixels))\n",
    "    \n",
    "        \n",
    "    return data\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def normalise(image, label):\n",
    "    \n",
    "    '''\n",
    "    To be used in preprocess_ML_tf\n",
    "    '''\n",
    "    \n",
    "    image = tf.cast(image, tf.int64)                 # Set dtype to int64\n",
    "    label = tf.cast(label, tf.int64)\n",
    "    #image /= np.amax(image.numpy())                    # Normalise Image\n",
    "    #image = tf.image.resize(image, (pixels, pixels, 1))   # Resize image to 40x40\n",
    "    return image, label\n",
    "\n",
    "def preprocess_ML_tf(data_s, data_b, batch_size):\n",
    "    \n",
    "    '''Prepares dataset for TensorFlow Machine Learning algorithm'''\n",
    "    \n",
    "    '''\n",
    "    Input1: Signal Dataset created using create_dataset\n",
    "    Input2: Background Dataset created using create_dataset\n",
    "    Input3: Batch size\n",
    "    \n",
    "    Process:\n",
    "    - Create labels for signals (1) and backgrounds (0)\n",
    "    - Combine signal and background datasets\n",
    "    - Combine signal and background labels\n",
    "    - Define useful (local) variables\n",
    "    - Reshape main dataset (for CNN)\n",
    "    - Train-Val-Test Split examples and labels\n",
    "    - Turn into tf datasets (train, val, split)\n",
    "    - Create Batches (train, val, split)\n",
    "    - Define useful (global) variables (returned later)\n",
    "    - Plot Events to Visualise & make sure everything's right (e.g. normalised vs non-normalised)\n",
    "    \n",
    "    Output1: train_batches\n",
    "    Output2: val_batches\n",
    "    Output3: test_batches\n",
    "    Output4: num_of_batches_train\n",
    "    Output5: num_of_batches_val\n",
    "    Output6: num_of_batches_test\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Preprocess'''\n",
    "    # Create s&b labels\n",
    "    slabels = np.ones(data_s.shape[0]//40)\n",
    "    blabels = np.zeros(data_b.shape[0]//40)\n",
    "\n",
    "    # Concatenate s&b and s&b labels\n",
    "    data = np.concatenate((data_s, data_b), axis=0)\n",
    "    labels = np.concatenate((slabels, blabels), axis=0)\n",
    "\n",
    "    # Define & Print useful variables (local)\n",
    "    num_of_examples = data.shape[0] // 40     # divide by 40 because 1st dim is 40 * num_of_examples\n",
    "    num_of_labels = labels.shape[0]\n",
    "    print('Total Events:', num_of_examples)\n",
    "    print('Total Labels:', num_of_labels)\n",
    "\n",
    "    # Reshape examples (for CNN)\n",
    "    examples = data.reshape(num_of_examples, 40, 40, 1)\n",
    "    print('Shape: ', examples.shape)\n",
    "    print(' ')\n",
    "    \n",
    "    \n",
    "    '''Train-Val-Test Split'''\n",
    "    train_examples, test_examples, train_labels, test_labels = train_test_split(examples, labels, test_size=0.15, random_state=42)\n",
    "    train_examples, val_examples, train_labels, val_labels = train_test_split(train_examples, train_labels, test_size=0.18, random_state=42)\n",
    "\n",
    "    print('Train: ', train_examples.shape, train_labels.shape)\n",
    "    print('Val: ', val_examples.shape, val_labels.shape)\n",
    "    print('Test: ', test_examples.shape, test_labels.shape)\n",
    "    print(' ')\n",
    "    \n",
    "    \n",
    "    train_data = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((val_examples, val_labels))\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))\n",
    "\n",
    "    print(train_data)\n",
    "    print(val_data)\n",
    "    print(test_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''BATCHES'''\n",
    "    batch_size = batch_size\n",
    "\n",
    "    train_batches = train_data.cache().shuffle(num_of_examples).map(normalise).batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "    val_batches = val_data.cache().shuffle(num_of_examples).map(normalise).batch(batch_size, drop_remainder=True).prefetch(1)  # or prefetch(buffer_size=tf.data.experimental.AUTOTUNE) \n",
    "    test_batches = test_data.cache().shuffle(num_of_examples).map(normalise).batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "    \n",
    "    # Define useful variables (will be returend)\n",
    "    num_of_batches_train = len(train_labels) // batch_size\n",
    "    num_of_batches_val = len(val_labels) // batch_size\n",
    "    num_of_batches_test = len(test_labels) // batch_size\n",
    "\n",
    "    print(train_batches)\n",
    "    print('\\ntrain, val, test: ', num_of_batches_train, num_of_batches_val, num_of_batches_test)\n",
    "    \n",
    "    \n",
    "    '''VISUALISE'''\n",
    "    plt.figure(figsize=(15,10))\n",
    "\n",
    "    for images, labels in train_batches.take(1):\n",
    "        for i in range(3):\n",
    "            ax = plt.subplot(3, 3, i + 1)\n",
    "            sns.heatmap(images[i].numpy().reshape(40, 40))\n",
    "            #plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "            plt.title('Image {}'.format(i+1))\n",
    "            plt.axis(\"off\")\n",
    "    \n",
    "    \n",
    "    return train_batches, val_batches, test_batches, num_of_batches_train, num_of_batches_val, num_of_batches_test\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def visualise_preds(model, test_batches):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Visualise Predictions for TensorFlow\n",
    "    \n",
    "    Input1: Model\n",
    "    Input 2: Predictions\n",
    "    Output: NULL\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''1st'''\n",
    "    \n",
    "    class_names = ['Background', 'Signal']\n",
    "\n",
    "    for event, label in test_batches.take(1):\n",
    "        ps = model.predict(event)\n",
    "        images = event.numpy().squeeze()\n",
    "        labels = label.numpy()\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "\n",
    "    for n in range(6):\n",
    "        plt.subplot(3,3,n+1)\n",
    "        sns.heatmap(images[n])\n",
    "        #plt.imshow(images[n], cmap = plt.cm.binary)\n",
    "        color = 'green' if np.argmax(ps[n]) == labels[n] else 'red'\n",
    "        plt.title(class_names[np.argmax(ps[n])], color=color)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        \n",
    "        \n",
    "    '''2nd'''\n",
    "    \n",
    "    for event, label in test_batches.take(1):\n",
    "        ps = model.predict(event)\n",
    "        first_image = event.numpy().squeeze()[0]\n",
    "\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "        #sns.heatmap(first_image)\n",
    "        ax1.imshow(first_image)\n",
    "        ax1.axis('off')\n",
    "        ax2.barh(np.arange(2), ps[0])\n",
    "        ax2.set_aspect(0.1)\n",
    "        ax2.set_yticks(np.arange(2))\n",
    "        ax2.set_yticklabels(np.arange(2))\n",
    "        ax2.set_title('Class Probability')\n",
    "        ax2.set_xlim(0, 1.1)\n",
    "        plt.tight_layout()\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def learning_curve(train_batches, val_batches, test_batches, num_of_batches_train, num_of_batches_val):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Plots a learning curve to determine whether more data would improve the model (i.e. detect underfitting)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    input_shape=(40, 40, 1)\n",
    "    kernel_size = 2\n",
    "    padding='valid'\n",
    "    activation = 'tanh'\n",
    "\n",
    "    prop = [0.1, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "    loss_list = []\n",
    "    accuracy_list = []\n",
    "\n",
    "    for i in prop:\n",
    "        model = tf.keras.Sequential([\n",
    "                      tf.keras.Input(shape=input_shape),\n",
    "                      tf.keras.layers.Conv2D(16, kernel_size=kernel_size, padding=padding, activation=activation),\n",
    "                      tf.keras.layers.MaxPooling2D(),\n",
    "                      tf.keras.layers.Conv2D(32, kernel_size=kernel_size, padding=padding, activation=activation),\n",
    "                      tf.keras.layers.MaxPooling2D(),\n",
    "                      tf.keras.layers.Conv2D(64, kernel_size=kernel_size, padding=padding, activation=activation),\n",
    "                      tf.keras.layers.MaxPooling2D(),\n",
    "                      tf.keras.layers.Flatten(),\n",
    "                      tf.keras.layers.Dense(128, activation=activation),\n",
    "                      tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "            ])\n",
    "\n",
    "\n",
    "        # Compile Model\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        print('\\n\\n', i, '\\n')\n",
    "\n",
    "        # Fit model to training data\n",
    "        EPOCHS = 4\n",
    "\n",
    "        history = model.fit(train_batches.take(int(i*num_of_batches_train)), \n",
    "                  epochs=EPOCHS,\n",
    "                  validation_data=val_batches.take(int(i*num_of_batches_val)), \n",
    "                  verbose=0\n",
    "                  )\n",
    "\n",
    "        loss, accuracy = model.evaluate(test_batches, verbose=0)\n",
    "        loss_list.append(loss)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "        loss, accuracy = model.evaluate(test_batches, verbose=0)\n",
    "        print('Accuracy on the Test Set: {:.1%}'.format(accuracy))\n",
    "        \n",
    "    plt.plot(np.array(prop)*100, accuracy_list, linestyle='--', marker='o')\n",
    "    plt.xlabel('% of Dataset Used')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.show()\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def model_complexity_graph(history):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Plots model complexity graph to determine how many epochs you need (i.e. when the model starts overfitting the data)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    training_accuracy = history.history['accuracy']\n",
    "    validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_range=range(len(training_accuracy))\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, training_accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, training_loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def cmx_tf(models, test_batches, num_of_batches_test):\n",
    "    \n",
    "    '''\n",
    "    Plots Confusion Matrix for TensorFlow list of models\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for model in models:\n",
    "    \n",
    "        '''Extract Preds & Labels'''\n",
    "\n",
    "        preds_all = []\n",
    "        labels_all = []\n",
    "        preds_batch = []\n",
    "        labels_batch = []\n",
    "\n",
    "\n",
    "        # For all batches\n",
    "        for batch, labels in test_batches.take(num_of_batches_test):\n",
    "\n",
    "            # 64 preds and labels added to list\n",
    "            pp = model.predict(batch)\n",
    "            preds_batch = np.array([np.argmax(pp[i]) for i in range(len(pp))])\n",
    "            labels_batch = labels.numpy()\n",
    "\n",
    "            preds_all.append(preds_batch)\n",
    "            labels_all.append(labels_batch)\n",
    "\n",
    "\n",
    "        # Convert list of lists to ndarray and flatten to get 1D ndarray of all preds and 1D ndarray of all labels\n",
    "        preds = np.array(preds_all).flatten()\n",
    "        labels = np.array(labels_all).flatten()      \n",
    "\n",
    "\n",
    "        '''Build CMX'''\n",
    "\n",
    "        cmx_non_normal = tf.math.confusion_matrix(labels, preds).numpy() # Create Confusion Matrix\n",
    "        cmx0 = cmx_non_normal[0] / cmx_non_normal[0].sum()\n",
    "        cmx1 = cmx_non_normal[1] / cmx_non_normal[1].sum()\n",
    "        cmx = np.stack((cmx0, cmx1), axis=0)\n",
    "\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(cmx, cmap=['skyblue', 'deepskyblue', 'dodgerblue', 'blue',  'darkblue'])\n",
    "\n",
    "        # xylabels and title\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('PREDICTIONS')\n",
    "        plt.ylabel('LABELS')\n",
    "\n",
    "        # Label ticks\n",
    "        ax.set_xticklabels(['Background', 'Signal'])\n",
    "        ax.set_yticklabels(['Background', 'Signal'])\n",
    "        # Align ticks\n",
    "        plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        plt.setp(ax.get_yticklabels(), rotation=90, ha=\"center\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "\n",
    "        # Text Annotations for Blocks in CMX\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "\n",
    "                value = int(np.round(100*cmx[i, j], 0))\n",
    "\n",
    "                text = ax.text(j+0.5, \n",
    "                               i+0.5, \n",
    "                               value,\n",
    "                               ha=\"center\", \n",
    "                               va=\"center\", \n",
    "                               color=\"orangered\", \n",
    "                               fontsize = 20)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        # # Print P(signal|signal) and P(signal|background)\n",
    "        # pss = cmx[1,1] / (cmx[1,1]+cmx[1,0])\n",
    "        # pbs = 1 - pss\n",
    "        # psb = cmx[0,1] / (cmx[0,1]+cmx[0,0])\n",
    "        # pbb = 1 - psb\n",
    "        # precision = cmx[1,1] / (cmx[1,1]+cmx[0,1])\n",
    "        # recall = cmx[1,1] / (cmx[1,1]+cmx[1,0])\n",
    "        # print('\\n')\n",
    "        # print('P(signal|signal) = {:.0f}%'.format(100*pss))\n",
    "        # print('P(signal|background) = {:.0f}%'.format(100*psb)) \n",
    "        # print('P(background|background) = {:.0f}%'.format(100*pbb))\n",
    "        # print('P(background|signal) = {:.0f}%'.format(100*pbs))\n",
    "        # print('Precision = {:.0f}'.format(precision*100))\n",
    "        # print('Recall = {:.0f}'.format(recall*100))\n",
    "        # print('\\n')\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def ROC3_tf(model0, model1, model2, test_batches, num_of_batches_test):\n",
    "    \n",
    "    ''' \n",
    "    Plot ROC curve for exactly 3 TensorFlow models\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''Extract Preds & Labels'''\n",
    "\n",
    "    preds_batch = []\n",
    "    labels_batch = []\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "\n",
    "\n",
    "    # For all batches\n",
    "    for batch, labels in test_batches.take(num_of_batches_test):\n",
    "\n",
    "        # 64 preds and labels added to list\n",
    "        pp = model0.predict(batch)\n",
    "        preds_batch = np.array([np.argmax(pp[i]) for i in range(len(pp))])\n",
    "        labels_batch = labels.numpy()\n",
    "\n",
    "        preds_all.append(preds_batch)\n",
    "        labels_all.append(labels_batch)\n",
    "\n",
    "\n",
    "    # Convert list of lists to ndarray and flatten to get 1D ndarray of all preds and 1D ndarray of all labels\n",
    "    preds0 = np.array(preds_all).flatten()\n",
    "    labels0 = np.array(labels_all).flatten()\n",
    "\n",
    "\n",
    "    ##########################################################################################################################################################################\n",
    "\n",
    "\n",
    "    preds_batch = []\n",
    "    labels_batch = []\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "\n",
    "\n",
    "    # For all batches\n",
    "    for batch, labels in test_batches.take(num_of_batches_test):\n",
    "\n",
    "    # 64 preds and labels added to list\n",
    "        pp = model1.predict(batch)\n",
    "        preds_batch = np.array([np.argmax(pp[i]) for i in range(len(pp))])\n",
    "        labels_batch = labels.numpy()\n",
    "\n",
    "        preds_all.append(preds_batch)\n",
    "        labels_all.append(labels_batch)\n",
    "\n",
    "\n",
    "    # Convert list of lists to ndarray and flatten to get 1D ndarray of all preds and 1D ndarray of all labels\n",
    "    preds1 = np.array(preds_all).flatten()\n",
    "    labels1 = np.array(labels_all).flatten()\n",
    "\n",
    "\n",
    "    ##########################################################################################################################################################################\n",
    "\n",
    "\n",
    "    preds_batch = []\n",
    "    labels_batch = []\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "\n",
    "\n",
    "    # For all batches\n",
    "    for batch, labels in test_batches.take(num_of_batches_test):\n",
    "\n",
    "    # 64 preds and labels added to list\n",
    "        pp = model2.predict(batch)\n",
    "        preds_batch = np.array([np.argmax(pp[i]) for i in range(len(pp))])\n",
    "        labels_batch = labels.numpy()\n",
    "\n",
    "        preds_all.append(preds_batch)\n",
    "        labels_all.append(labels_batch)\n",
    "\n",
    "\n",
    "    # Convert list of lists to ndarray and flatten to get 1D ndarray of all preds and 1D ndarray of all labels\n",
    "    preds2 = np.array(preds_all).flatten()\n",
    "    labels2 = np.array(labels_all).flatten()     \n",
    "\n",
    "\n",
    "    '''Build ROC'''\n",
    "        \n",
    "    ##########################################################################################################################################################################\n",
    "\n",
    "\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "    fpr0, tpr0, thresholds = roc_curve(labels0, preds0)\n",
    "    auc0 = auc(fpr0, tpr0)\n",
    "\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(labels1, preds1)\n",
    "    auc1 = auc(fpr1, tpr1)\n",
    "\n",
    "    fpr2, tpr2, thresholds2 = roc_curve(labels2, preds2)\n",
    "    auc2 = auc(fpr2, tpr2)\n",
    "\n",
    "    ##########################################################################################################################################################################\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr0, tpr0, label='0 (area = {:.3f})'.format(auc0))\n",
    "    plt.plot(fpr1, tpr1, label='1 (area = {:.3f})'.format(auc1))\n",
    "    plt.plot(fpr2, tpr2, label='2 (area = {:.3f})'.format(auc2))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "#     # Zoom in view of the upper left corner.\n",
    "#     plt.figure(2)\n",
    "#     plt.xlim(0, 0.2)\n",
    "#     plt.ylim(0.8, 1)\n",
    "#     plt.plot([0, 1], [0, 1], 'k--')\n",
    "#     plt.plot(fpr0, tpr0, label='0 (area = {:.3f})'.format(auc0))\n",
    "#     plt.plot(fpr1, tpr1, label='1 (area = {:.3f})'.format(auc1))\n",
    "#     plt.plot(fpr2, tpr2, label='1 (area = {:.3f})'.format(auc2))\n",
    "#     plt.xlabel('False positive rate')\n",
    "#     plt.ylabel('True positive rate')\n",
    "#     plt.title('ROC curve (zoomed in at top left)')\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.show()\n",
    "\n",
    "    ##########################################################################################################################################################################\n",
    "'''\n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN SKLEARN \n",
    "'''\n",
    "    \n",
    "def preprocess_ML_sklearn(data_s, data_b):\n",
    "    \n",
    "    '''Prepares dataset for sklearn Machine Learning algorithm'''\n",
    "    \n",
    "    '''\n",
    "    Input1: Signal Dataset created using create_dataset\n",
    "    Input2: Background Dataset created using create_dataset\n",
    "    \n",
    "    Process:\n",
    "    - Create labels for signals (1) and backgrounds (0)\n",
    "    - Combine signal and background datasets\n",
    "    - Combine signal and background labels\n",
    "    - Define useful (local) variables\n",
    "    - Reshape main dataset (for sklearn)\n",
    "    - Train-Val-Test Split examples and labels\n",
    "    - Plot Events to Visualise & make sure everything's right (e.g. normalised vs non-normalised)\n",
    "    \n",
    "    Output1: train_examples\n",
    "    Output2: train_examples\n",
    "    Output3: val_examples\n",
    "    Output4: val_labels\n",
    "    Output5: test_examples\n",
    "    Output6: test_labels\n",
    "    '''\n",
    "    \n",
    "    # Create s&b labels\n",
    "    slabels = np.ones(data_s.shape[0]//40)\n",
    "    blabels = np.zeros(data_b.shape[0]//40)\n",
    "\n",
    "    # Concatenate examples and labels\n",
    "    data = np.concatenate((data_s, data_b), axis=0)\n",
    "    labels = np.concatenate((slabels, blabels), axis=0)\n",
    "\n",
    "    # Define useful quantities\n",
    "    num_of_examples = data.shape[0] // 40     # divide by 40 because 1st dim is 40 * num_of_examples\n",
    "    num_of_labels = labels.shape[0]\n",
    "    print('Total Events:', num_of_examples)\n",
    "    print('Total Labels:', num_of_labels)\n",
    "\n",
    "    # Reshape examples (for sklearn)\n",
    "    examples = data.reshape(num_of_examples, 1600)\n",
    "    print('\\nShape: ', examples.shape)\n",
    "\n",
    "    train_examples, test_examples, train_labels, test_labels = train_test_split(examples, labels, test_size=0.15, random_state=42)\n",
    "    train_examples, val_examples, train_labels, val_labels = train_test_split(train_examples, train_labels, test_size=0.18, random_state=42)\n",
    "\n",
    "    print('\\nTrain: ', train_examples.shape, train_labels.shape)\n",
    "    print('Val: ', val_examples.shape, val_labels.shape)\n",
    "    print('Test: ', test_examples.shape, test_labels.shape)\n",
    "    print(' ')\n",
    "\n",
    "    eg = train_examples[5].reshape(40, 40)\n",
    "\n",
    "    sns.heatmap(eg)\n",
    "    plt.title(\"Train Image Example\")\n",
    "    plt.show()\n",
    "    \n",
    "    return train_examples, train_labels, val_examples, val_labels, test_examples, test_labels\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def compare_f1(models, test_examples, test_labels):\n",
    "    \n",
    "    '''Plots f1-score of Models using Test Data and prints top 5 models'''\n",
    "    \n",
    "    '''\n",
    "    Input1: list of models\n",
    "    Input2: test examples\n",
    "    Input3: test labels\n",
    "    \n",
    "    Output1: NULL\n",
    "    '''\n",
    "    \n",
    "    # Local Variables\n",
    "    model_names = []\n",
    "    scores = []\n",
    "\n",
    "    # Get plot data\n",
    "    for model in models:\n",
    "\n",
    "        labels = test_labels\n",
    "        preds = model.predict(test_examples)\n",
    "        scores.append(f1_score(labels, preds))\n",
    "        model_names.append(model.__class__.__name__)\n",
    "\n",
    "    # Make Plots\n",
    "    fig, ax = plt.subplots(figsize=(27, 6))\n",
    "    plt.bar(model_names, scores, color=\"darkcyan\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print top 5 algorithms\n",
    "    max_i = np.flip(np.argsort(scores))\n",
    "    print('F1score')\n",
    "    for i in max_i:\n",
    "        print('{:.4f} {}'.format(scores[i], model_names[i])) \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def compare_accuracy(models, test_examples, test_labels):\n",
    "    \n",
    "    '''Plots f1-score of Models using Test Data and prints top 5 models'''\n",
    "    \n",
    "    '''\n",
    "    Input1: list of models\n",
    "    Input2: test examples\n",
    "    Input3: test labels\n",
    "    \n",
    "    Output1: NULL\n",
    "    '''\n",
    "    \n",
    "    # Local Variables\n",
    "    model_names = []\n",
    "    scores = []\n",
    "\n",
    "    # Get plot data\n",
    "    for model in models:\n",
    "\n",
    "        labels = test_labels\n",
    "        preds = model.predict(test_examples)\n",
    "        scores.append(accuracy_score(labels, preds))\n",
    "        model_names.append(model.__class__.__name__)\n",
    "\n",
    "    # Make Plots\n",
    "    fig, ax = plt.subplots(figsize=(27, 6))\n",
    "    plt.bar(model_names, scores, color=\"darkcyan\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print top 5 algorithms\n",
    "    max_i = np.flip(np.argsort(scores))\n",
    "    print('Accuracy')\n",
    "    for i in max_i:\n",
    "        print('{:.4f} {}'.format(scores[i], model_names[i]))  \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def cmx_sklearn(models, test_examples, test_labels, dim=4):\n",
    "    \n",
    "    '''\n",
    "    Plots Confusion Matrix for sklearn list of models\n",
    "    \n",
    "    '''\n",
    "    cmxs = []\n",
    "    \n",
    "    for model in models:\n",
    "            \n",
    "            preds = model.predict(test_examples)\n",
    "            labels = test_labels\n",
    "\n",
    "            cmx_non_normal = tf.math.confusion_matrix(labels, preds).numpy() # Create Confusion Matrix\n",
    "            cmx0 = cmx_non_normal[0] / cmx_non_normal[0].sum()\n",
    "            cmx1 = cmx_non_normal[1] / cmx_non_normal[1].sum()\n",
    "            cmx = np.stack((cmx0, cmx1), axis=0)\n",
    "            cmxs.append(cmx)\n",
    "\n",
    "    plt.figure(figsize=(25,20))\n",
    "    for n in range(len(cmxs)):\n",
    "        # Plot confusion matrix\n",
    "        ax = plt.subplot(dim, dim, n+1)\n",
    "        sns.heatmap(cmxs[n], cmap=['skyblue', 'deepskyblue', 'dodgerblue', 'blue',  'darkblue'])\n",
    "\n",
    "        # xylabels and title\n",
    "        plt.title(remove_text_inside_brackets(str(models[n])))\n",
    "        plt.xlabel('PREDICTIONS')\n",
    "        plt.ylabel('LABELS')\n",
    "\n",
    "        # Label ticks\n",
    "        ax.set_xticklabels(['Background', 'Signal'])\n",
    "        ax.set_yticklabels(['Background', 'Signal'])\n",
    "        # Align ticks\n",
    "        plt.setp(ax.get_xticklabels(), rotation=0, ha=\"center\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        plt.setp(ax.get_yticklabels(), rotation=90, ha=\"center\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "\n",
    "        # Text Annotations for Blocks in CMX\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "\n",
    "                value = int(np.round(100*cmxs[n][i, j], 0))\n",
    "\n",
    "                text = ax.text(j+0.5, \n",
    "                               i+0.5, \n",
    "                               value,\n",
    "                               ha=\"center\", \n",
    "                               va=\"center\", \n",
    "                               color=\"orangered\", \n",
    "                               fontsize = 20)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()      \n",
    "    print(cmxs)\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def learning_curve_sklearn(models, train_examples, train_labels, val_examples, val_labels):\n",
    "    \n",
    "    total_train = train_labels.shape[0]\n",
    "    total_val = val_labels.shape[0]\n",
    "    print('Total No. of Training Examples:', total_train)\n",
    "    \n",
    "    props = [0.1, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for model in models:\n",
    "        print('\\n=================================================================================================================================================================================\\n')\n",
    "        accuracy = []\n",
    "        times = []\n",
    "        \n",
    "        for prop in props:\n",
    "            prop_examples = int(prop*total_train)\n",
    "            #print('Current No. of Training Examples:', prop_examples)\n",
    "            start = time.time()\n",
    "            model.fit(train_examples[0:prop_examples], train_labels[0:prop_examples])\n",
    "            end = time.time()\n",
    "            times.append((end-start)/60)\n",
    "            #print('Training time for {} examples: {:.3f} minutes'.format(total_train, (end-start)/60))\n",
    "            start = time.time()\n",
    "            prop_examples_val = int(prop*total_val)\n",
    "            val_preds = model.predict(val_examples[0:prop_examples_val])\n",
    "            accuracy.append(accuracy_score(val_labels[0:prop_examples_val], val_preds[0:prop_examples_val]))\n",
    "            end = time.time()\n",
    "            #print('Prediction time for {} examples: {:.3f} minutes\\n'.format(total_val, (end-start)/60))\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        plt.plot(np.array(props)*100, accuracy, linestyle='--', marker='o')\n",
    "        plt.xlabel('% of Dataset Used')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('{} - Learning Curve'.format(model.__class__.__name__))\n",
    "        plt.show()\n",
    "        \n",
    "        plt.plot(np.array(props)*100, times, linestyle='--', marker='o')\n",
    "        plt.xlabel('% of Dataset Used')\n",
    "        plt.ylabel('Training Time (minutes)')\n",
    "        plt.title('{} - Training Time'.format(model.__class__.__name__))\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Proportions:\", props)\n",
    "        print(\"Accuracy: {}\".format(accuracy))\n",
    "        print(\"Time: {}\".format(times))\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    \n",
    "def create_dataset_sklearn(file, pixels=40, R=1.5):\n",
    "    '''\n",
    "    Takes dat file of events\n",
    "    Labels events (background = 0, signal = 1)\n",
    "    Preprocessed events and turns into images\n",
    "    Returns 2d array where rows: events and columns: (image, label) \n",
    "    '''\n",
    "\n",
    "    data = ((0, 0))\n",
    "    image = np.zeros((pixels, pixels))                           # Define initial image\n",
    "    \n",
    "        \n",
    "    if file=='data_background.dat':\n",
    "        label = 0\n",
    "    elif file=='data_signal.dat':\n",
    "        label = 1\n",
    "    else: \n",
    "        print(\"ERROR: File name unclear\")\n",
    "        return\n",
    "    \n",
    "    with open(file) as infile:\n",
    "        for line in infile:\n",
    "\n",
    "            # Preprocessing\n",
    "            event = line.strip().split()\n",
    "            event = pd.Series(event)                         # Turn into Series\n",
    "            event = preprocess(event)                        # Preprocess\n",
    "            max1 = find_max1(event)                          # Extract maxima\n",
    "            event = center(event, max1)                      # Center \n",
    "            max2 = find_max2(event)                          # Extract maxima\n",
    "            event = rotate(event, max2)                      # Rotate \n",
    "            max3 = find_max3(event)                          # Extract maxima\n",
    "            event = flip(event, max3)                        # Flip \n",
    "            event = create_image(event, pixels=pixels, R=R)  # Create image\n",
    "            #event = event.flatten()                          # Flatten image from 2D to 1D for NN\n",
    "            image = event                                   # Rename\n",
    "            #image /= np.amax(image)                          # Normalise final image between 0 and 1\n",
    "            \n",
    "            event=max1=max2=max3=None                            # Delete from memory\n",
    "\n",
    "            event = np.array((image, label))\n",
    "            data = np.vstack((data, event))\n",
    "    \n",
    "    data = np.delete(data, 0, axis=0)\n",
    "    return data\n",
    "'''\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING PREPROCESSING\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "   \n",
    "                    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def average_image_1(pixels=60, R=1.5, event_no=12178, display=False, file='data/dataset_s_100k.dat'):\n",
    "    '''\n",
    "    Reads events directly from a file and creates an average image of the events. \n",
    "    \n",
    "    pixels: int. Image Resolution\n",
    "    R: float. Fat jet radius\n",
    "    event_no: int/list. Number of events for which images are created. If int, then single image (faster). If list, then multiple images (slower)\n",
    "    display: boolean. Indicates whether images should be displayed automatically (return null) or returned as an ndarray. \n",
    "    '''\n",
    "\n",
    "    image = np.zeros((pixels, pixels))                           # Define initial image\n",
    "    a = 0                                                        # Define Counter\n",
    "    \n",
    "    \n",
    "    #Return single image\n",
    "    if type(event_no) == int:\n",
    "        \n",
    "        with open(file) as infile:\n",
    "            for line in infile:\n",
    "\n",
    "                event = line.strip().split()\n",
    "                event = pd.Series(event)                         # Turn into Series\n",
    "                event = preprocess(event)                        # Preprocess\n",
    "                #max1 = find_max1(event)           # Extract maxima\n",
    "                #event = center(event, max1)                    # Center \n",
    "                #max2 = find_max2(event)\n",
    "                #event = rotate(event, max2)                   # Rotate \n",
    "                #max3 = find_max3(event)\n",
    "                #event = flip(event, max3)                     # Flip \n",
    "                event = create_image(event, pixels=pixels, R=R)  # Create image\n",
    "                image += event                                   # Add event image to average image\n",
    "                #image /= np.amax(image)                          # Normalise final image between 0 and 1\n",
    "                event=max1=max2=max3=None                            # Delete from memory\n",
    "\n",
    "                a += 1\n",
    "                if a == event_no:                                 # Break if max sample size for average image is exceeded \n",
    "                    return image\n",
    "def average_image_2(pixels=60, R=1.5, event_no=12178, display=False, file='data/dataset_s_100k.dat'):\n",
    "    '''\n",
    "    Reads events directly from a file and creates an average image of the events. \n",
    "    \n",
    "    pixels: int. Image Resolution\n",
    "    R: float. Fat jet radius\n",
    "    event_no: int/list. Number of events for which images are created. If int, then single image (faster). If list, then multiple images (slower)\n",
    "    display: boolean. Indicates whether images should be displayed automatically (return null) or returned as an ndarray. \n",
    "    '''\n",
    "\n",
    "    image = np.zeros((pixels, pixels))                           # Define initial image\n",
    "    a = 0                                                        # Define Counter\n",
    "    \n",
    "    \n",
    "    #Return single image\n",
    "    if type(event_no) == int:\n",
    "        \n",
    "        with open(file) as infile:\n",
    "            for line in infile:\n",
    "\n",
    "                event = line.strip().split()\n",
    "                event = pd.Series(event)                         # Turn into Series\n",
    "                event = preprocess(event)                        # Preprocess\n",
    "                max1 = find_max1(event)           # Extract maxima\n",
    "                event = center(event, max1)                    # Center \n",
    "#                 max2 = find_max2(event)\n",
    "#                 event = rotate(event, max2)                   # Rotate \n",
    "#                 max3 = find_max3(event)\n",
    "#                 event = flip(event, max3)                     # Flip \n",
    "                event = create_image(event, pixels=pixels, R=R)  # Create image\n",
    "                image += event                                   # Add event image to average image\n",
    "                #image /= np.amax(image)                          # Normalise final image between 0 and 1\n",
    "                event=max1=max2=max3=None                            # Delete from memory\n",
    "\n",
    "                a += 1\n",
    "                if a == event_no:                                 # Break if max sample size for average image is exceeded \n",
    "                    return image\n",
    "def average_image_3(pixels=60, R=1.5, event_no=12178, display=False, file='data/dataset_s_100k.dat'):\n",
    "    '''\n",
    "    Reads events directly from a file and creates an average image of the events. \n",
    "    \n",
    "    pixels: int. Image Resolution\n",
    "    R: float. Fat jet radius\n",
    "    event_no: int/list. Number of events for which images are created. If int, then single image (faster). If list, then multiple images (slower)\n",
    "    display: boolean. Indicates whether images should be displayed automatically (return null) or returned as an ndarray. \n",
    "    '''\n",
    "\n",
    "    image = np.zeros((pixels, pixels))                           # Define initial image\n",
    "    a = 0                                                        # Define Counter\n",
    "    \n",
    "    \n",
    "    #Return single image\n",
    "    if type(event_no) == int:\n",
    "        \n",
    "        with open(file) as infile:\n",
    "            for line in infile:\n",
    "\n",
    "                event = line.strip().split()\n",
    "                event = pd.Series(event)                         # Turn into Series\n",
    "                event = preprocess(event)                        # Preprocess\n",
    "                max1 = find_max1(event)           # Extract maxima\n",
    "                event = center(event, max1)                    # Center \n",
    "                max2 = find_max2(event)\n",
    "                event = rotate(event, max2)                   # Rotate \n",
    "#                 max3 = find_max3(event)\n",
    "#                 event = flip(event, max3)                     # Flip \n",
    "                event = create_image(event, pixels=pixels, R=R)  # Create image\n",
    "                image += event                                   # Add event image to average image\n",
    "                #image /= np.amax(image)                          # Normalise final image between 0 and 1\n",
    "                event=max1=max2=max3=None                            # Delete from memory\n",
    "\n",
    "                a += 1\n",
    "                if a == event_no:                                 # Break if max sample size for average image is exceeded \n",
    "                    return image\n",
    "def average_image_4(pixels=60, R=1.5, event_no=12178, display=False, file='data/dataset_s_100k.dat'):\n",
    "    '''\n",
    "    Reads events directly from a file and creates an average image of the events. \n",
    "    \n",
    "    pixels: int. Image Resolution\n",
    "    R: float. Fat jet radius\n",
    "    event_no: int/list. Number of events for which images are created. If int, then single image (faster). If list, then multiple images (slower)\n",
    "    display: boolean. Indicates whether images should be displayed automatically (return null) or returned as an ndarray. \n",
    "    '''\n",
    "\n",
    "    image = np.zeros((pixels, pixels))                           # Define initial image\n",
    "    a = 0                                                        # Define Counter\n",
    "    \n",
    "    \n",
    "    #Return single image\n",
    "    if type(event_no) == int:\n",
    "        \n",
    "        with open(file) as infile:\n",
    "            for line in infile:\n",
    "\n",
    "                event = line.strip().split()\n",
    "                event = pd.Series(event)                         # Turn into Series\n",
    "                event = preprocess(event)                        # Preprocess\n",
    "                max1 = find_max1(event)           # Extract maxima\n",
    "                event = center(event, max1)                    # Center \n",
    "                max2 = find_max2(event)\n",
    "                event = rotate(event, max2)                   # Rotate \n",
    "                max3 = find_max3(event)\n",
    "                event = flip(event, max3)                     # Flip \n",
    "                event = create_image(event, pixels=pixels, R=R)  # Create image\n",
    "                image += event                                   # Add event image to average image\n",
    "                #image /= np.amax(image)                          # Normalise final image between 0 and 1\n",
    "                event=max1=max2=max3=None                            # Delete from memory\n",
    "\n",
    "                a += 1\n",
    "                if a == event_no:                                 # Break if max sample size for average image is exceeded \n",
    "                    return image\n",
    "\n",
    "\n",
    "\n",
    "def average_image(pixels=60, R=1.5, event_no=12178, display=False, file='data/dataset_s_100k.dat'):\n",
    "    '''\n",
    "    Reads events directly from a file and creates an average image of the events. \n",
    "    \n",
    "    pixels: int. Image Resolution\n",
    "    R: float. Fat jet radius\n",
    "    event_no: int/list. Number of events for which images are created. If int, then single image (faster). If list, then multiple images (slower)\n",
    "    display: boolean. Indicates whether images should be displayed automatically (return null) or returned as an ndarray. \n",
    "    '''\n",
    "\n",
    "    image = np.zeros((pixels, pixels))                           # Define initial image\n",
    "    a = 0                                                        # Define Counter\n",
    "    \n",
    "    \n",
    "    #Return single image\n",
    "    if type(event_no) == int:\n",
    "        \n",
    "        with open(file) as infile:\n",
    "            for line in infile:\n",
    "\n",
    "                event = line.strip().split()\n",
    "                event = pd.Series(event)                         # Turn into Series\n",
    "                event = preprocess(event)                        # Preprocess\n",
    "                max1 = find_max1(event)           # Extract maxima\n",
    "                event = center(event, max1)                    # Center \n",
    "                max2 = find_max2(event)\n",
    "                event = rotate(event, max2)                   # Rotate \n",
    "                max3 = find_max3(event)\n",
    "                event = flip(event, max3)                     # Flip \n",
    "                event = create_image(event, pixels=pixels, R=R)  # Create image\n",
    "                image += event                                   # Add event image to average image\n",
    "                #image /= np.amax(image)                          # Normalise final image between 0 and 1\n",
    "                event=max1=max2=max3=None                            # Delete from memory\n",
    "\n",
    "                a += 1\n",
    "                if a == event_no:                                 # Break if max sample size for average image is exceeded \n",
    "                    return image\n",
    "\n",
    "                    \n",
    "                \n",
    "    \n",
    "    # Display Images\n",
    "    elif display == True and type(event_no) == list:\n",
    "                \n",
    "        with open(file) as infile:\n",
    "            for line in infile:\n",
    "\n",
    "                event = line.strip().split()\n",
    "                event = pd.Series(event)                         # Turn into Series\n",
    "                event = preprocess(event)                        # Preprocess\n",
    "                max1 = find_max1(event)           # Extract maxima\n",
    "                event = center(event, max1)                    # Center \n",
    "                max2 = find_max2(event)\n",
    "                event = rotate(event, max2)                   # Rotate \n",
    "                max3 = find_max3(event)\n",
    "                event = flip(event, max3)                     # Flip \n",
    "                event = create_image(event, pixels=pixels, R=R)  # Create image\n",
    "                image += event                                   # Add event image to average image\n",
    "                #image /= np.amax(image)                          # Normalise final image between 0 and 1\n",
    "                event=max1=max2=max3=None                            # Delete from memory\n",
    "                \n",
    "                a += 1\n",
    "                if a in event_no:\n",
    "                    sns.heatmap(image, robust=True)\n",
    "                    plt.show()\n",
    "#                     sns.heatmap(image)\n",
    "#                     plt.show()\n",
    "                    if a >= max(event_no):                        # Break if max sample size for average image is exceeded \n",
    "                        break\n",
    "                    \n",
    "    \n",
    "        \n",
    "    # Return multiple images\n",
    "    ##### Not working properly\n",
    "    elif type(event_no) == list:\n",
    "        images = []                                         # List containing the output images\n",
    "        \n",
    "        with open(file) as infile:\n",
    "            for line in infile:\n",
    "\n",
    "                event = line.strip().split()\n",
    "                event = pd.Series(event)                         # Turn into Series\n",
    "                event = preprocess(event)                        # Preprocess\n",
    "                max1 = find_max1(event)           # Extract maxima\n",
    "                event = center(event, max1)                    # Center \n",
    "                max2 = find_max2(event)\n",
    "                event = rotate(event, max2)                   # Rotate \n",
    "                max3 = find_max3(event)\n",
    "                event = flip(event, max3)                     # Flip \n",
    "                event = create_image(event, pixels=pixels, R=R)  # Create image\n",
    "                image += event                                   # Add event image to average image\n",
    "                #image /= np.amax(image)                          # Normalise final image between 0 and 1\n",
    "                event=max1=max2=max3=None                            # Delete from memory\n",
    "\n",
    "                a += 1\n",
    "                if a in event_no:                                 # Store images\n",
    "                    images.append(image)\n",
    "                    if a >= max(event_no):                        # Break if max sample size for average image is exceeded\n",
    "                        return images\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(event):\n",
    "    '''\n",
    "    Input: Series (event) to be processed\n",
    "    Output: Preprocessed Series\n",
    "    \n",
    "    -Drops constituents element\n",
    "    -Replaces NaN values with 0\n",
    "    -Converts all values to floats\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # Drop constituents \n",
    "    event = event.drop(event.index[0])\n",
    "    \n",
    "    # Replace NaN with 0\n",
    "    event = event.fillna(0)\n",
    "\n",
    "    # Convert values to floats\n",
    "    event = event.astype(float)\n",
    "    \n",
    "    return event\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_max1(event):\n",
    "\n",
    "    '''\n",
    "    Takes an event and outputs a tuple containing 3 Series, each for the highest pT and its φ, η.\n",
    "    \n",
    "    Input: Series (event). \n",
    "\n",
    "    Output[0]: [Series of 1st max pT, φ, η]\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Separate η, φ, pT\n",
    "    hdata = event[::3]\n",
    "    fdata = event[1::3]\n",
    "    pdata = event[2::3]\n",
    "\n",
    "\n",
    "    # 1. Extract index of 1st maximum pT\n",
    "    maxid1 = pdata.idxmax()\n",
    "    maxlist1 = []\n",
    "\n",
    "    # 2. Extract max η, φ, pT for event\n",
    "    if pdata.max() != 0:                                                                     # Brief explanation of if statement below)\n",
    "        maxlist1.append([event.iloc[maxid1-1], event.iloc[maxid1-2], event.iloc[maxid1-3]])   # From event, add to list the max pT and its η, φ\n",
    "    else:\n",
    "        maxlist1.append([0., event.iloc[maxid1-2], event.iloc[maxid1-3]])                    # If max pT is 0, then add it as 0 and not the first value\n",
    "\n",
    "    # 3. Create series of max pT, η, φ\n",
    "    max1 = pd.Series(data=maxlist1[0], index=['pT', 'φ', 'η'])\n",
    "\n",
    "    return max1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def find_max2(event):\n",
    "    \n",
    "    '''\n",
    "    Takes an event and outputs a tuple containing 3 Series, each for the highest pT and its φ, η.\n",
    "    \n",
    "    Input: Series (event). \n",
    "    Output: [Series of 2nd max pT, φ, η]\n",
    "    '''\n",
    "    \n",
    "    # Separate η, φ, pT\n",
    "    hdata = event[::3]\n",
    "    fdata = event[1::3]\n",
    "    pdata1 = event[2::3]\n",
    "\n",
    "\n",
    "    # 0. 1st pT = 0 to find 2nd Max pT\n",
    "    pdata = pdata1.copy(deep=True)\n",
    "    pdata.loc[pdata.idxmax()] = 0\n",
    "\n",
    "    # 1. Extract index of 2nd max pT\n",
    "    maxid2 = pdata.idxmax()\n",
    "    maxlist2 = []\n",
    "    \n",
    "    # Extract numerical index of φ of 2nd max pT\n",
    "    f_id_2 = maxid2 - 1      \n",
    "    h_id_2 = maxid2 - 2\n",
    "    \n",
    "    \n",
    "\n",
    "    # 2. Extract max η, φ, pT for event\n",
    "    if pdata.max() != 0:                                                                     # Brief explanation of if statement below)\n",
    "        maxlist2.append([event.iloc[maxid2-1], event.iloc[maxid2-2], event.iloc[maxid2-3]])   # From event, add to list the max pT and its η, φ\n",
    "    else:\n",
    "        maxlist2.append([0., event.iloc[maxid2-2], event.iloc[maxid2-3]])                    # If max pT is 0, then add it as 0 and not the first value\n",
    "\n",
    "    # 3. Create series of max pT, η, φ\n",
    "    max2 = pd.Series(data=maxlist2[0], index=['pT', 'φ', 'η'])\n",
    "    \n",
    "    return max2\n",
    "    \n",
    "    \n",
    "    \n",
    "def find_max3(event):\n",
    "    \n",
    "    '''\n",
    "    Takes an event and outputs a Series containing the 3rd highest pT, and its φ, η\n",
    "    \n",
    "    Input: Series (event). \n",
    "    Output: [Series of 3rd max pT, φ, η]\n",
    "    '''\n",
    "\n",
    "    # Separate η, φ, pT\n",
    "    hdata = event[::3]\n",
    "    fdata = event[1::3]\n",
    "    pdata1 = event[2::3]\n",
    "\n",
    "\n",
    "    # 0. 1st, 2nd pT = 0 to find 3rd Max pT\n",
    "    pdata = pdata1.copy(deep=True)\n",
    "    pdata.loc[pdata.idxmax()] = 0\n",
    "    pdata.loc[pdata.idxmax()] = 0\n",
    "\n",
    "\n",
    "    # 1. Extract index of 3rd max pT\n",
    "    maxid3 = pdata.idxmax()\n",
    "    maxlist3 = []\n",
    "    \n",
    "\n",
    "\n",
    "    # 2. Extract max η, φ, pT for event\n",
    "    if pdata.max() != 0:                                                                     # Brief explanation of if statement below)\n",
    "        maxlist3.append([event.iloc[maxid3-1], event.iloc[maxid3-2], event.iloc[maxid3-3]])   # From event, add to list the max pT and its η, φ\n",
    "    else:\n",
    "        maxlist3.append([0., event.iloc[maxid3-2], event.iloc[maxid3-3]])                    # If max pT is 0, then add it as 0 and not the first value\n",
    "\n",
    "    # 3. Create series of max pT, η, φ\n",
    "    max3 = pd.Series(data=maxlist3[0], index=['pT', 'φ', 'η'])\n",
    "\n",
    "    return  max3\n",
    "\n",
    "# **Why the if statement?** (note to self) <br />\n",
    "# Because if maximum pT is 0 in the pdata vector, it picks the ID of the first pT by default as the max (because they're all 0). <br />\n",
    "# Then, it goes to the non-zero'd event vector and adds its non-zero pT as the max, when the value of that max should clearly have been 0.\n",
    "\n",
    "# So the if statement fixes this: <br />\n",
    "# - If max pT != 0, then add it as normal.\n",
    "# - If max pT = 0, then add '0' as its value instead. (with the coordinates of the first pT, which is incorrect, but this doesn't matter since pT = 0 are not taken into account in the image) <br />     \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def center(event, max1, output='event', R=1.5, pixels=60):\n",
    "    \n",
    "    '''\n",
    "    Centers image around (φ', η') = (0, 0). Both transformations are linear (so far). \n",
    "    \n",
    "    event1: Series (event)\n",
    "    max123: Tuple of 3 series of max pT, η, φ. Returned by extract_max123() function\n",
    "    output: 'event' to return a Series of the transformed event1. 'image' to return a transformed dataframe representing an image \n",
    "    '''\n",
    "    \n",
    "    # Define η, φ indices to be used later\n",
    "    h_indices = event[::3].index\n",
    "    f_indices = event[1::3].index\n",
    "\n",
    "    \n",
    "    \n",
    "    # For all η, φ in the event\n",
    "    for h_index, f_index in zip(h_indices, f_indices):             \n",
    "\n",
    "        # Define Useful Quantities\n",
    "        num_index = event.name         # REDUNTANT? REMOVE IT. index of event, so that we can find its corresponding φ in the max123[0] dataframe of max pT's and φ, η's\n",
    "        maxh = max1.loc['η']                # η of max1 pT value\n",
    "        maxf = max1.loc['φ']                # φ of max1 pT value\n",
    "        f = event.iloc[1::3][f_index]            # φ original value\n",
    "        \n",
    "        # η Transformation\n",
    "        event.iloc[::3][h_index] -= maxh         # Subtract max η from current η\n",
    "        \n",
    "        # φ Transformation (Note: the if statements take periodicity into account, making sure that range does not exceed 2π)\n",
    "        if (f - maxf) < -np.pi:\n",
    "            event.iloc[1::3][f_index] = f + 2*np.pi - maxf\n",
    "\n",
    "        elif (f - maxf) > np.pi:\n",
    "            event.iloc[1::3][f_index] = f - 2*np.pi - maxf\n",
    "\n",
    "        else: \n",
    "            event.iloc[1::3][f_index] -= maxf     # Subtract max φ from current φ\n",
    "\n",
    "\n",
    "    if output == 'event':\n",
    "        return event\n",
    "    \n",
    "    \n",
    "    elif output == 'image':\n",
    "        # Initiate bin lists\n",
    "        bin_h = []\n",
    "        bin_f = []\n",
    "        bin_p = []\n",
    "\n",
    "        # Define max number of constituents \n",
    "        max_const = event.shape[0] // 3\n",
    "        # For all constituents\n",
    "        for i in range(max_const):\n",
    "            # Add constituent's η, φ, p to bins\n",
    "            bin_h.append(list(event.iloc[::3])[i])\n",
    "            bin_f.append(list(event.iloc[1::3])[i])\n",
    "            bin_p.append(list(event.iloc[2::3])[i])\n",
    "\n",
    "        # Turn lists into Series\n",
    "        bin_h = pd.Series(bin_h)\n",
    "        bin_f = pd.Series(bin_f)\n",
    "        bin_p = pd.Series(bin_p)\n",
    "\n",
    "        # Define no. of bins\n",
    "        bin_count = np.linspace(-R, R, pixels + 1)\n",
    "\n",
    "        # Create bins from -R to R and convert to DataFrame\n",
    "        bins = np.histogram2d(bin_h, bin_f, bins=bin_count, weights=bin_p)[0] # x and y are switch because when the bins were turned into a Series the shape[0] and shape[1] were switched\n",
    "        image = bins\n",
    "        \n",
    "        return image\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rotate(event, max2):\n",
    "    '''\n",
    "    Input: Series (event), max2 series obtained from find_max2()\n",
    "    Output: Rotated Series (event)\n",
    "    \n",
    "    -Calculates the angle of rotation so that φ of 2nd highest pT ends up at (φ', η') = (0, h) for some h > 0 \n",
    "    -Transforms all η and φ of event using the formulas below (from mathematics) \n",
    "    (η' = ηcosθ + φsinθ)\n",
    "    (φ' = φcosθ - ηsinθ)\n",
    "    => θ = arctan(φ/η), with if statements taking care of η = 0 cases and making sure η' is positive and not negative\n",
    "    '''\n",
    "\n",
    "    # Calculate Angle\n",
    "    hmax=max2.loc['η']\n",
    "    fmax=max2.loc['φ']\n",
    "    \n",
    "    angle = 0\n",
    "    \n",
    "    if (hmax == 0) and (fmax > 0):\n",
    "        angle = np.pi/2\n",
    "    elif (hmax == 0) and (fmax < 0):\n",
    "        angle = -np.pi/2\n",
    "    elif hmax > 0:\n",
    "        angle = np.arctan(fmax/hmax)\n",
    "    elif hmax < 0:\n",
    "        angle = np.arctan(fmax/hmax) + np.pi\n",
    "        \n",
    "\n",
    "    # Rotate Image\n",
    "    h_indices = event[::3].index\n",
    "    f_indices = event[1::3].index\n",
    "    \n",
    "\n",
    "    for h_index, f_index in zip(h_indices, f_indices): \n",
    "        \n",
    "        h = event.iloc[0::3][h_index]\n",
    "        f = event.iloc[1::3][f_index]\n",
    "        \n",
    "        event.iloc[1::3][f_index] = f*np.cos(angle) - h*np.sin(angle)\n",
    "        event.iloc[::3][h_index] = f*np.sin(angle) + h*np.cos(angle) \n",
    "    \n",
    "        \n",
    "    return event\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flip(event, max3):\n",
    "    '''\n",
    "    Input: Series (event), max3 series obtained from find_max3()\n",
    "    Output: Flipped Series (event)\n",
    "    \n",
    "    -Checks if φ is on left-hand side\n",
    "    -If yes, it multiplies all φ with -1 to flip the image\n",
    "    '''\n",
    "    \n",
    "    # Check if 2nd highest pT is on left-hand side\n",
    "    if max3.loc['φ'] < 0:\n",
    "        \n",
    "        # Define φ indices for transformation\n",
    "        f_indices = event[1::3].index\n",
    "        \n",
    "        # For all φ \n",
    "        for f_index in f_indices: \n",
    "            # Multiply φ by -1\n",
    "            event.iloc[1::3][f_index] *= -1\n",
    "    \n",
    "    return event\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_image(event, R=1.5, pixels=60):\n",
    "    \n",
    "    '''    \n",
    "    Creates an image of single event.\n",
    "    \n",
    "    Input: Series (event)\n",
    "    Output: ndarray (image)\n",
    "    '''\n",
    "    \n",
    "    # Turn into DataFrame\n",
    "    event = pd.DataFrame(event).T\n",
    "    \n",
    "    # Initiate bin lists\n",
    "    bin_h = []\n",
    "    bin_f = []\n",
    "    bin_p = []\n",
    "\n",
    "        \n",
    "    # Add constituent's coordinates to bin lists\n",
    "    const = event.shape[1] // 3     # For all constituents\n",
    "    for i in range(const):\n",
    "        bin_h.append(list(event.iloc[0][::3])[i])\n",
    "        bin_f.append(list(event.iloc[0][1::3])[i])\n",
    "        bin_p.append(list(event.iloc[0][2::3])[i])\n",
    "\n",
    "\n",
    "    # Turn lists into Series\n",
    "    bin_h = pd.Series(bin_h)\n",
    "    bin_f = pd.Series(bin_f)\n",
    "    bin_p = pd.Series(bin_p)\n",
    "\n",
    "   # Define number & range of bins\n",
    "    bin_count = np.linspace(-R, R, pixels + 1)\n",
    "\n",
    "    # Create image (array)\n",
    "    bins = np.histogram2d(bin_h, bin_f, bins=bin_count, weights=bin_p)[0] # x and y are switch because when the bins were turned into a Series the shape[0] and shape[1] were switched\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    image = bins\n",
    "    \n",
    "    return image\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_text_inside_brackets(text, brackets=\"()[]\"):\n",
    "    count = [0] * (len(brackets) // 2) # count open/close brackets\n",
    "    saved_chars = []\n",
    "    for character in text:\n",
    "        for i, b in enumerate(brackets):\n",
    "            if character == b: # found bracket\n",
    "                kind, is_close = divmod(i, 2)\n",
    "                count[kind] += (-1)**is_close # `+1`: open, `-1`: close\n",
    "                if count[kind] < 0: # unbalanced bracket\n",
    "                    count[kind] = 0  # keep it\n",
    "                else:  # found bracket to remove\n",
    "                    break\n",
    "        else: # character is not a [balanced] bracket\n",
    "            if not any(count): # outside brackets\n",
    "                saved_chars.append(character)\n",
    "    return ''.join(saved_chars)\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
