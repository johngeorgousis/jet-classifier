{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠êJonas Questions\n",
    "- \n",
    "- \n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠êImports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "from processing_functions import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠êStep 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¥ Define helper function that\n",
    "- drops the constituents column \n",
    "- converts NaN to 0\n",
    "- converts values to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠êStep 2: Create Image & Average Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîµ Create Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¥ Define Helper Function that takes an event as input and returns an image\n",
    "- Bins coordinates (Œ∑, œÜ, pT)\n",
    "- Creates image using np.histogram2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_image(event, R=1.5, pixels=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîµ Create Average Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¥ Define Helper Function that reads events directly from a file and returns an average image\n",
    "\n",
    "\n",
    "**NOTE:** event_no list implementation for multiple images is not working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average_image(pixels=60, R=1.5, event_no=12178, display=False):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (Image Progression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average_image(pixels = 100, event_no=[25, 300, 3000, 12000], display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠êStep 3: Extract Maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¥ Define 3 Helper Functions that \n",
    "\n",
    "Return one vector each with the highest pT and its Œ∑, œÜ. (For the three maximum pT's)\n",
    "\n",
    "- **1st function**: 1st maximum pT and its Œ∑, œÜ\n",
    "- **2nd function**: 2nd maximum pT and its Œ∑, œÜ\n",
    "- **3rd function**: 3rd maximum pT and its Œ∑, œÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_max1(event)\n",
    "#find_max2(event)\n",
    "#find_max3(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠êStep 4: Centre Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row, we centre a new coordinate system so that the highest pT constituent's coordinates are (œÜ', Œ∑') = (0, 0). <br />\n",
    "This corresponds to rotating and boosting along the beam direction to center the jet.\n",
    "\n",
    "**œÜ Tranformation**<br />\n",
    "For the œÜ transformation, we subtract the œÜ (of the max pT) from all œÜ's in that row. <br />\n",
    "If the values exceed [-œÄ, œÄ], we add 2œÄ to the final result (if it's <-œÄ) or subtract 2œÄ from the final result (if it's >œÄ). This makes sure that no values exceed the original œÜ interval. <br />\n",
    "This has the effect of making the œÜ (corresponding to the max pT for that row) equal to 0 in each row, and shifting the other œÜ's by that same angle, while maintaining a range of 2œÄ. <br />\n",
    "\n",
    "**Œ∑ Transformation**<br />\n",
    "How does Œ∑ transform? We need a Lorentz Transformation. \n",
    "\n",
    "**Paper** (E) <br />\n",
    "Histograms binned in\n",
    "either the angular separation of events or the rapidity separation of events can\n",
    "be contributed to by events whose centre of mass frames are boosted by arbitrary velocities with respect to the rest frame of the detector, the lab frame.\n",
    "The resulting histograms are undistorted by these centre of mass frame boosts\n",
    "parallel to the beam axis, as the dependent variable is invariant with respect\n",
    "to this sub‚Äìclass of Lorentz boosts.\n",
    "\n",
    "**Paper** (F): make code cell below markdown to display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<img src=\"h1.png\" width=\"500\"> <img src=\"h2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¥ Define Helper Function <br />\n",
    "Centers image around (œÜ', Œ∑') = (0, 0). Both transformations are linear (so far). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#center(event, max123, output='event', R=1.5, pixels=60):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠êStep 5: Rotate Image\n",
    "\n",
    "Rotate all constituents around (œÜ‚Äô,Œ∑‚Äô)=0 such that the constituent with the 2nd highest pT is at 12 o‚Äôclock, i.e. at  (œÜ‚Äô,Œ∑‚Äô)=(0,e) with e > 0.\n",
    "\n",
    "**Paper (C)** <br />\n",
    "\"Rotation: Rotation is performed to remove the stochastic nature of the decay\n",
    "angle relative to the Œ∑ ‚àí œÜ coordinate system.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¥ Define Helper Function that\n",
    "\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rotate(event, max2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠êStep 6: Flip Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flip all the constituents such that the constituents with the 3rd highest pT is on the right-half plane, i.e. at (œÜ‚Äô,Œ∑‚Äô)=(f,e) with f > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî¥ Define Helper Function that\n",
    "\n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flip(event, max3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠ê Building Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥ Import Data\n",
    "\n",
    "1. Import signal/background data (~7min)\n",
    "2. Concatenate & Shuffle signal/background data\n",
    "3. Convert to tf.data.Dataset object\n",
    "\n",
    "\n",
    "**Save Dataset** **NOTE: PICKLE IMPLEMENTATION DOES NOT WORK YET**\n",
    "\n",
    "The easiest way is to pickle it using to_pickle:\n",
    "\n",
    "`df.to_pickle(file_name)`, where to save it, usually as a .pkl\n",
    "\n",
    "Then you can load it back using:\n",
    "\n",
    "`df = pd.read_pickle(file_name)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥ Load s&b data (~7mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Previously saved data\n",
    "# sdata = pd.read_pickle('sdata')\n",
    "# bdata = pd.read_pickle('bdata')\n",
    "\n",
    "# Import, Preprocess, Create Dataset\n",
    "sdata = create_dataset('data_signal.dat')\n",
    "bdata = create_dataset('data_background.dat')\n",
    "\n",
    "# Save Datasets\n",
    "pd.DataFrame(sdata).to_pickle('sdata')\n",
    "pd.DataFrame(sdata).to_pickle('bdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥ Combine and Shuffle s&b data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Events: 29449\n"
     ]
    }
   ],
   "source": [
    "# Concat and Suffle\n",
    "data = np.concatenate((sdata, bdata), axis=0)\n",
    "data = sklearn.utils.shuffle(data)\n",
    "\n",
    "num_of_examples = data.shape[0]\n",
    "print('Total Events:', num_of_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥ (PROBLEM HERE) Convert to tf.data.Dataset Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Examples and Labels\n",
    "data= pd.DataFrame(data, columns=['examples', 'labels'])\n",
    "examples= data['examples']         \n",
    "labels = data['labels']            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "1        [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "2        [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "3        [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "4        [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "                               ...                        \n",
       "29444    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "29445    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "29446    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "29447    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "29448    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "Name: examples, Length: 29449, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        1\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "29444    0\n",
       "29445    0\n",
       "29446    0\n",
       "29447    0\n",
       "29448    1\n",
       "Name: labels, Length: 29449, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into ndarray to convert to tf.data.Dataset object\n",
    "examples = np.array(examples)    # ndarray of images\n",
    "labels = np.array(labels)        # ndarray of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tf.data.Dataset object\n",
    "data = tf.data.Dataset.from_tensor_slices(([examples], [labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29449,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29449,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "       array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "       array([[0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "       ...,\n",
       "       [0.00000000e+000, 0.00000000e+000, 8.36934867e-127, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000]]),\n",
       "       ...,\n",
       "       array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "       array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]]),\n",
       "       array([[0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "       ...,\n",
       "       [0.00000000e+000, 0.00000000e+000, 8.36934867e-127, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000],\n",
       "       [0.00000000e+000, 0.00000000e+000, 0.00000000e+000, ...,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000]])],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 1], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(examples)\n",
    "print('----------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(data.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî¥ (PROBLEM HERE: shapes are the same for all splits) Create training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20614 4417 4417\n",
      "<TakeDataset shapes: ((29449, 40, 40), (29449,)), types: (tf.float64, tf.int32)>\n",
      "<SkipDataset shapes: ((29449, 40, 40), (29449,)), types: (tf.float64, tf.int32)>\n",
      "<TakeDataset shapes: ((29449, 40, 40), (29449,)), types: (tf.float64, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.7 * num_of_examples)\n",
    "val_size = int(0.15 * num_of_examples)\n",
    "test_size = int(0.15 * num_of_examples)\n",
    "\n",
    "print(train_size, val_size, test_size)\n",
    "\n",
    "train_data = data.take(train_size)\n",
    "test_data = data.skip(train_size)\n",
    "val_data = test_data.skip(val_size)\n",
    "test_data = test_data.take(test_size)\n",
    "\n",
    "print(train_data)\n",
    "print(val_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "\n",
    "# # Convert .dat files into DataFrame objects\n",
    "# sdata = pd.DataFrame([i.strip().split() for i in open(\"tth_semihad.dat\").readlines()])\n",
    "# bdata = pd.DataFrame([i.strip().split() for i in open(\"wjets.dat\").readlines()])\n",
    "\n",
    "# # Label data\n",
    "# sdata['label'] = np.ones(sdata.shape[0])\n",
    "# bdata['label'] = np.zeros(bdata.shape[0])\n",
    "\n",
    "# # Concatenate & Shuffle\n",
    "# data = pd.concat([sdata, bdata])\n",
    "# data = shuffle(data)\n",
    "\n",
    "# # Extract labels & drop\n",
    "# labels = np.array(data['label'])\n",
    "# data = data.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî¥ Visualise Data  \n",
    "(TRY REMOVING NORMALISATION FROM PREPROCESSING (create_dataset), THEN DISPLAY IMAGE, THEN WHEN CREATING BATCHES USE .MAP(NORMALISE) TO NORMALISE IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 18 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for images, labels in train_data.take(1):\n",
    "      for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        sns.heatmap(images[i].numpy().astype(\"uint8\"))\n",
    "        #plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(i)\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üî¥ Create Batches\n",
    "\n",
    "- `Dataset.prefetch()` overlaps data preprocessing and model execution while training.\n",
    "\n",
    "- `Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n",
    "\n",
    "Theory guide: https://www.tensorflow.org/guide/data_performance#prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-60215a10e730>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-60215a10e730>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    -\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(image, label):\n",
    "    image = tf.cast(image, tf.int64)                 # Set dtype to int64\n",
    "    label = tf.cast(label, tf.int64)\n",
    "    #image /= np.amax(image.numpy())                    # Normalise Image\n",
    "    #image = tf.image.resize(image, (pixels, pixels))   # Resize image to 40x40\n",
    "    return image, label\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_batches = train_data.cache().shuffle(num_of_examples//4).map(normalise).batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "val_batches = val_data.cache().map(normalise).batch(batch_size, drop_remainder=True).prefetch(1)  # or prefetch(buffer_size=tf.data.experimental.AUTOTUNE) \n",
    "\n",
    "test_batches = test_data.cache().map(normalise).batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "\n",
    "list(data.as_numpy_iterator())\n",
    "train_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching Testing (IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_batches:\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.shape)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "\n",
    "dataset = dataset.batch(3, drop_remainder=True).prefetch(1)\n",
    "\n",
    "print(list(dataset.as_numpy_iterator()))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_data.batch(3, drop_remainder=True).prefetch(1)\n",
    "\n",
    "print(list(train_batches.as_numpy_iterator()))\n",
    "train_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥ Build the Model\n",
    "\n",
    "Look up ConvNet TF Tutorial and make a Conv2D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "pixels = 40\n",
    "\n",
    "input_shape=(batch_size, pixels, pixels)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "          tf.keras.Input(shape=input_shape),\n",
    "          tf.keras.layers.Conv2D(16, kernel_size=2, padding='same', activation='relu'),\n",
    "          tf.keras.layers.MaxPooling2D(),\n",
    "          tf.keras.layers.Conv2D(32, kernel_size=2, padding='same', activation='relu'),\n",
    "          tf.keras.layers.MaxPooling2D(),\n",
    "          tf.keras.layers.Conv2D(64, kernel_size=2, padding='same', activation='relu'),\n",
    "          tf.keras.layers.MaxPooling2D(),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          tf.keras.layers.Dense(128, activation='relu'),\n",
    "          tf.keras.layers.Dense(2, activation = 'softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥ Compile the Model\n",
    "- **Optimizer** -- Determines how the model is updated based on the data it sees and its loss function\n",
    "- **Loss Function** -- Measures how accurate the model is during training\n",
    "- **Metrics** -- Monitors the training and testing steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥ Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early Stopping**: <br /> \n",
    "Stop training when there is no improvement in the validation loss for 5 consecutive epochs or when metric gain is less than 0.001\n",
    "\n",
    "**Save Best**: <br />\n",
    "Saves the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, min_delta = 0.001)\n",
    "save_best = tf.keras.callbacks.ModelCheckpoint('./best_model.h5', monitor='loss', save_best_only=True)\n",
    "\n",
    "# Fit model to training data\n",
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(train_batches, \n",
    "          epochs=EPOCHS,\n",
    "          callbacks= [early_stopping, save_best],\n",
    "          validation_data=val_batches, \n",
    "          verbose=2, \n",
    "          steps_per_epoch=10\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model to training data\n",
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(train_data, \n",
    "          epochs=EPOCHS,\n",
    "          callbacks= [early_stopping, save_best],\n",
    "          validation_data=val_data, \n",
    "          verbose=2, \n",
    "          steps_per_epoch=10\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model-Complexity Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "training_loss = history.history['loss']\n",
    "validation_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range=range(len(training_accuracy))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, training_accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, training_loss, label='Training Loss')\n",
    "plt.plot(epochs_range, validation_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "\n",
    "# loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "# epochs_range = range(EPOCHS)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "# plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(epochs_range, loss, label='Training Loss')\n",
    "# plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, accuracy = my_model.evaluate(test_data, verbose=2)\n",
    "\n",
    "# print('\\nLoss on the TEST Set: {:,.3f}'.format(loss))\n",
    "# print('Accuracy on the TEST Set: {:.3%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥ Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event, label in test_data.take(1):\n",
    "#     ps = model.predict(event)\n",
    "#     images = event.numpy().squeeze()\n",
    "#     labels = label.numpy()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10,15))\n",
    "\n",
    "# for n in range(30):\n",
    "#     plt.subplot(6,5,n+1)\n",
    "#     plt.imshow(images[n], cmap = plt.cm.binary)\n",
    "#     color = 'green' if np.argmax(ps[n]) == labels[n] else 'red'\n",
    "#     plt.title(class_names[np.argmax(ps[n])], color=color)\n",
    "#     plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event, label in training_batches.take(1):\n",
    "#     ps = model.predict(event)\n",
    "#     first_image = event.numpy().squeeze()[0]\n",
    "  \n",
    "  \n",
    "# fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "# ax1.imshow(first_image, cmap = plt.cm.binary)\n",
    "# ax1.axis('off')\n",
    "# ax2.barh(np.arange(10), ps[0])\n",
    "# ax2.set_aspect(0.1)\n",
    "# ax2.set_yticks(np.arange(10))\n",
    "# ax2.set_yticklabels(np.arange(10))\n",
    "# ax2.set_title('Class Probability')\n",
    "# ax2.set_xlim(0, 1.1)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥ Build Confusion Matrix \n",
    "\n",
    "Guide:\n",
    "https://kapernikov.com/tutorial-image-classification-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
